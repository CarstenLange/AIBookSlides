---
title: "Polynomial Regression"
subtitle: "Overlearning, Training, Testing, and Validation"
format: 
  revealjs:
    code-fold: true
---

## Overview

You will learn about:

::: nonincremental

- Overlearning in detail.

- Circumstances that make *overlearning* more  likely to occur.

- Consequences of overlearning when predicting new data.

- Hyper-parameter tuning to avoid *overlearning*.

  - Validation
  - Cross Validation
    
:::
    
## Overlearning

If a model performs well when approximating the training data but does not perform well when it faces new data to predict outcomes.

Overlearning is one of the most pressing and still not fully solved problems in machine learning.

## Circumstances that Can Lead to Overlearning


- If the training dataset does not have a sufficient number of observations.

- If the model considers many variables and thus contains many parameters to calibrate.

- If the underlying machine learning model is highly non-linear.

## The Data {.scrollable}

We use the Kings County Real Estate dataset.

```{r}
#| echo: true
library(tidymodels); library(rio); library(janitor)

DataHousing = import("https://lange-analytics.com/AIBook/Data/HousingData.csv")%>%
  clean_names("upper_camel") %>%
  select(Price, Sqft=SqftLiving)
```

We want to demonstrate *overlearning* this is the reason. Therefore, we work only with a very small training dataset (20 observations/0.1% of the total observations. All other observations become testing data: 
```{r}
#| echo: true
set.seed(777)
# initial_split(prop = 0.001, ...) randomly chooses 20 training observations
Split001=DataHousing %>% 
  initial_split(prop = 0.001, strata = Price, breaks = 5) 
DataTrain=training(Split001)
DataTest=testing(Split001)
```

## The Data

There seems to be a non-liner trend:
```{r}
ggplot(aes(y=Price, x=Sqft), data=DataHousing)+
  geom_point(size=0.5)+
  geom_smooth(se=FALSE)
```

## Polynomial Regression

Regular univariate prediction equation:
$$
\widehat{Price}=\beta_1 Sqft+\beta_2
$$

Polynomial univariate prediction equation (degree 5):
$$\widehat{Price}=\beta_1 Sqft+\beta_2 Sqft^2+\beta_3 Sqft^3+\beta_4 Sqft^4+\beta_5 Sqft^5+\beta_6$$


$$
\widehat{Price}=\beta_1 Sqft+\beta_2 Sqft^2+\beta_3 Sqft^3+\beta_4 Sqft^4+\beta_5 Sqft^5+\beta_6
$$


## Polynomial Regression

Polynomial univariate prediction equation (degree 5):
$$\widehat{Price}=\beta_1 Sqft+\beta_2 Sqft^2+\beta_3 Sqft^3+\beta_4 Sqft^4+\beta_5 Sqft^5+\beta_6$$

We create $Sqft^2$, $Sqft^3$, $Sqft^4$, and $Sqft^5$ as new variables in the data and treat them as they were separate variables in a multivariate regression. 

. . . 

This makes the regression **linear in variables** but **non-linear in data**.»

. . .

Consequently we can OLS to find the optimal $\beta s$. 

## Comparing Regular OLS and Polynominal Regression (degree=5)

Code to compare is linked in the footer of this slide.

::: footer
See: [RandRStudioScript.R script100](https://lange-analytics.com/AIBook/Scripts/TrainTest.R) 
:::

## Polynomial Regression (degree=5) vs. Regular OLS 
### Aproximation of the Training Data

```{r}
ModelDesignLinRegr=linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

RecipeHousesBenchmOLS=recipe(Price ~ Sqft, data=DataTrain)

RecipeHousesPolynomOLS=recipe(Price ~ ., data=DataTrain) %>% 
  step_mutate(Sqft2=Sqft^2,Sqft3=Sqft^3,Sqft4=Sqft^4,Sqft5=Sqft^5)

######## Fitted Workflows
WFModelHousesBenchmOLS=workflow() %>% 
  add_model(ModelDesignLinRegr) %>% 
  add_recipe(RecipeHousesBenchmOLS) %>% 
  fit(DataTrain)

WFModelHousesPolynomOLS=workflow() %>% 
  add_model(ModelDesignLinRegr) %>% 
  add_recipe(RecipeHousesPolynomOLS) %>% 
  fit(DataTrain)

####### Generating Predictions #######

DataTrainWithPredBenchmOLS=augment(WFModelHousesBenchmOLS, DataTrain)
DataTrainWithPredPolynomOLS=augment(WFModelHousesPolynomOLS, DataTrain)

DataTestWithPredBenchmOLS=augment(WFModelHousesBenchmOLS, DataTest)
DataTestWithPredPolynomOLS=augment(WFModelHousesPolynomOLS, DataTest)

########## Polynomial degree 10

RecipeHousesPolynom10OLS=recipe(Price ~ ., data=DataTrain) %>% 
  step_poly(Sqft, degree = 10, options = list(raw = TRUE))
  
  
WFModelHousesPolynom10OLS=workflow() %>% 
  add_model(ModelDesignLinRegr)%>%            
  add_recipe(RecipeHousesPolynom10OLS) %>% 
  fit(DataTrain)

DataTrainWithPredPolynom10OLS=augment(WFModelHousesPolynom10OLS, DataTrain)
DataTestWithPredPolynom10OLS=augment(WFModelHousesPolynom10OLS, DataTest)
```



## Polynomial Regression (degree=5) vs. Regular OLS 
### Aproximation of the Training Data

```{r}

ggplot(aes(x=Sqft,y=Price), data=DataTrainWithPredPolynomOLS)+
  geom_smooth(method = "lm", se=FALSE, size=1)+
  geom_point(color="red")+
  geom_line(aes(y=.pred), color="magenta", size=1, data=DataTestWithPredPolynomOLS)+
  xlim(c(500,4500))+
  ylim(c(0,1500000))
```

## Polynomial Regression (degree=5) vs. Regular OLS {.smaller}
### Training and Testing Data Performance

```{r}
ggplot(aes(x=Sqft,y=Price), data=DataTestWithPredPolynomOLS)+
  geom_point(size=0.001)+
  geom_smooth(method = "lm", se=FALSE, size=1, data=DataTrainWithPredPolynomOLS)+
  geom_line(aes(y=.pred), color="magenta", size=1)+
  geom_point(color="red", size=2, data=DataTrainWithPredPolynomOLS)+
  xlim(c(0,4500))+
  ylim(c(0,1500000))
```

$$\widehat{Price}=\beta_1 Sqft+\beta_2 Sqft^2+\beta_3 Sqft^3 + +\beta_4 Sqft^4 +\beta_5 Sqft^5 +\beta_{6}$$

## Polynomial Regression (degree=10) vs. Regular OLS {.smaller}
### Training and Testing Data Performance

```{r}
ggplot(aes(x=Sqft,y=Price), data=DataTestWithPredPolynom10OLS)+
  geom_point(size=0.001)+
  geom_smooth(method = "lm", se=FALSE, size=1, data=DataTrainWithPredPolynom10OLS)+
  geom_line(aes(y=.pred), color="magenta", size=1.2)+
  geom_point(color="red", size=2, data=DataTrainWithPredPolynom10OLS)+
  xlim(c(0,4300))+
  ylim(c(-150000,2000000))
```

$$\widehat{Price}=\beta_1 Sqft+\beta_2 Sqft^2+\beta_3 Sqft^3 +  \cdots +\beta_{10} Sqft^{10}+\beta_{11}$$

## Hyper-Parameters

- Parameters other than the $\beta$ parameters, because they can not be optimized by the *optimizer*.

- Hyper-Parameters are like settings for a machine learning model such as the number of polynomials (e.g. $Sft^N$ to be considered for polynomial regrssion or the number for $k$ k Nearest Neighbors.

- Hyper parameters often make a model more or leess complex and thus influence the uality of th training but also the chance of overlearning.»
