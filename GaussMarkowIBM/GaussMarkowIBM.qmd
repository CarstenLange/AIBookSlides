---
title: "Gauss Markov Assumptions"
subtitle: "Linearity, no perfect multicollinearity, strict exogeneity, no heteroscedacisity, no autocorrelation"
format: 
  revealjs:
    multiplex: true
    code-fold: false
    scrollable: true
    echo: true
    incremental: false
    smaller: false
---

## What Will You Learn {.scrollable .smaller}

-   What are the Gauss Markov assumptions?

-   What means *BLUE* (**B**est **L**inear **U**nbiased **E**stimator Model)?

-   How to test if Gauss Markov assumptions are violated?

-   What are the consequences if Gauss Markov assumptions are violated?

-   Remedies to use when Gauss Markov assumptions are violated?

## Gauss Markov Assumptions 

::: {.incremental}
-   Linearity

-   No perfect Multicollinearity,

-   Strict Exogeneity (errors are not correlated with any predictor variable's values)

-   No Heteroscedacisity

-   No Autocorrelation (errors are not correlated with their own lagged values; applies only when observations have a "natural order")
:::

## Best Linear Unbiased Estimator {.incremental}

If Gauss Markov assumptions are fulfilled a linear regression has the following properties:

::: {.incremental}
- **B**est: Lowest variance of error terms.

- **L**inear model

- **U**nbiased: The expected values of the sample  $\beta s$ coincide with the true $\beta s$ from the unobserved population. There is no systematic over- or under-estimation of the $\beta s$. If the $\beta s$ are only unbiased for big samples, we call the estimation *consistent*.

- **E**stimators are the $\beta s$
:::

## wage2 Data from Wooldridge --- Data Enginering

```{r}
#| output-location: fragment
#| code-line-numbers: "1-3|4-6|7|8|10"
library(tidymodels)
library(janitor)
library(wooldridge)
DataWage=wage2 |> 
         clean_names("upper_camel") |> 
         select(Wage,LogWage=Lwage, Educ, Tenure, RaceBl=Black, Feduc, Meduc) |> 
         mutate(ParEdu=pmax(Feduc,Meduc)) |> 
         drop_na(ParEdu) 

cat("Mean monthly wage:",mean(DataWage$Wage))
```

. . .

```{r}
#| eval: false
library(SmartEDA)
ExpReport(DataWage, op_file = "EdaDataWage2")
```

[Click to open: EdaDataWage2.html](EdaDataWage2.html "Click")

## Linearity

**If not fulfilled:** Coefficients $(\beta s)$ are biased and inconsistent

**Identify:** Visual inspection of regression and residuals.

**Remedy:** Use logarithms or polynomials (such as squares) on data.

## Linearity

```{r}
#| code-fold: true
DataPlot=DataWage |> 
         select(LogWage,Educ) |> 
         group_by(Educ) |> 
         mutate(CondMean=mean(LogWage))

ggplot(DataPlot, aes(y=LogWage, x=Educ))+
  geom_point(alpha=0.2)+
  geom_smooth(method="lm", se=FALSE)+
  geom_point(aes(x=Educ, y=CondMean), color="red", size=7)+
  scale_x_continuous(breaks = c(0:100))
  
```

## Multi Collinearity (Overview) {.smaller}

One or more of the independent variables are correlated.

**If not fulfilled (perfect multicolinearity):** Coefficients $(\beta s)$ cannot be calculated

**If not fulfilled (strong multicolinearity):** 

- Coefficients $(\beta s)$ are biased and inconsistent. 

- However, predictions are not.

- Coefficients $(\beta s)$ are sensitive to small changes in data.

**Identify:** VIF between an independent variables $i$ regressed on all other variables should be less than $10$ (some authors use less than 5): $VIF_i=\frac{1}{1-R^2_{i}}$

**Remedy:** 

- Do nothing

- Eliminate variable(s) that are correlated with others --- step wise (!)

- Others

## Perfect Multi Collinearity (Example) {.smaller}



Assume we have two variables for education. 

- $Edu$ measures the years of education, 

- $EducMonth$ measures the month of education and is 12 times $Educ$

```{r}
#| code-line-numbers: "1-8|9|11"
#| output-location: fragment
library(tidymodels)
library(janitor)
library(wooldridge)
DataWage=wage2 |> 
         clean_names("upper_camel") |> 
         select(Wage,LogWage=Lwage, Educ, Tenure, RaceBl=Black, Feduc, Meduc) |> 
         mutate(ParEdu=pmax(Feduc,Meduc)) |> 
         drop_na(ParEdu) |> 
         mutate(EducMonth=12*Educ)
           
lm(LogWage~ Educ+ EducMonth + Tenure+ RaceBl, DataWage)
```



## Strong Multi Collinearity (Example) {.smaller}



Assume we have two variables for education. 

- $Edu$ measures the years of education

- $EducMod$ 

    - counts high school dropouts as 12 years of education and 

    - does not consider graduate studies and thus cuts off $EducMod$ at 16 years.

```{r}
#| code-line-numbers: "1-8|9,10|12-14"
#| output-location: slide
library(tidymodels)
library(janitor)
library(wooldridge)
DataWage=wage2 |> 
         clean_names("upper_camel") |> 
         select(Wage,LogWage=Lwage, Educ, Tenure, RaceBl=Black, Feduc, Meduc) |> 
         mutate(ParEdu=pmax(Feduc,Meduc)) |> 
         drop_na(ParEdu) |> 
         mutate(EducMod=ifelse(Educ<12, 12, Educ)) |> 
         mutate(EducMod=ifelse(Educ>16, 16, EducMod))

ggplot(DataWage, aes(x=Educ, y=EducMod))+
      geom_point()+
      geom_smooth(method = "lm", se=FALSE)
```

## Strong Multi Collinearity (Regression Examples) {.smaller}

```{r}
#| code-line-numbers: "1|2|3|4"
ModelWage0=lm(LogWage~Educ+Tenure+RaceBl, DataWage)
set.seed(123)
ModelWage1=lm(LogWage~Educ+EducMod+Tenure+RaceBl, DataWage |> sample_n(500))
ModelWage2=lm(LogWage~Educ+EducMod+Tenure+RaceBl, DataWage |> sample_n(500))
```

::: {.fragment}
```{r}
summary(ModelWage0)
```
:::

::: {.fragment}
```{r}
summary(ModelWage1)
```
:::

::: {.fragment}
```{r}
summary(ModelWage2)
```
:::

::: {.fragment}
```{r}
library(car)
vif(ModelWage1)
vif(ModelWage2)
```
:::

::: {.fragment}
```{r}
ModelWage00=lm(LogWage~EducMod+Tenure+RaceBl, DataWage)
summary(ModelWage00)
```
:::

## Endogenity (Overview) {.smaller}

**Endogenity:** Error term is correlated with one or more of the independent (predictor) variables.


**If not fulfilled:** Coefficients $(\beta s)$ are biased and inconsistent. 


**Identify:** Difficult, because if estimate is biased errors are also biased and cannot be used to test for correlation with predictors.

If domain knowledge indicates endegenity, try IV Regression and see the results of the *Durbin-Wu-Hausmann* (*DWH*) test.

**Remedy:** 

- Do nothing

- Run two stage IV Regression

## Endogenity (Example)


```{r}
#| output-location: slide
library(ggdag)
set.seed(123)
dagify(Wage~Educ+Ability+RaceBl,
       Educ~Ability,
       Educ~RaceBl) |> ggdag(node_size = 22)
       
```

## Endogeneity (Correlation with the errors) --- Simple Model

$$\widehat{Wage}= \beta_1 Educ + \beta_2 RaceBl +\beta3$$




## Endogeneity (Correlation with the errors) --- IV Model 

### Instrumen: $ParEduc$

```{r}
#| echo: false
set.seed(123)
dagify(Wage~Educ+Ability+RaceBl,
       Educ~Ability,
       Educ~ParEdu,
       Educ~RaceBl) |> ggdag(node_size = 22)
```

## Endogeneity (Correlation with the error) --- IV Model 

### (Instrument: ParEduc) 2-Step Regression

. . .

$$
\widehat{Educ}= \alpha_1 ParEdu + \alpha_2
$$

. . .

$$
\widehat{Wage}= \beta_1 \widehat{Educ} + \beta_2 RaceBl +\beta3
$$


## Endogeneity --- IV Model

### Instrument: $ParEdu$


::: {.fragment}


```{r}
#| code-line-numbers: "1|2"
library(AER)
ModelWageEdiIV=ivreg(LogWage~RaceBl+Educ|RaceBl+ParEdu, data=DataWage)
```
:::
::: {.fragment}
```{r}
summary(ModelWageEdiIV)
```
:::


::: {.fragment}
```{r}
#| code-line-numbers: "1"
# Basic LM Model as Benchmark
ModelWageEdu=lm(LogWage~RaceBl+Educ,DataWage)
summary(ModelWageEdu)
```
:::


## Heteroscedasticity 

### Overview

**If not fulfilled:** Standard errors and thus P values are incorrect.

**Identify:** 

- Visual inspection of residuals. 

- White test.

**Remedy:** 
- White robust standard errors

- Generalized Least Square (weighs observation with variance, if it is known.)

## Heteroscedasticity 

Error variance is not constant for different values of dependent variable.

### No Heteroscedasticity in IV Wage Model

```{r}
#| code-fold: true
DataPlot=augment(ModelWageEdiIV)

ggplot(DataPlot, aes(x=,.fitted, y=.resid))+
  geom_point(alpha=0.2)+
  labs(x = "LogWage (estimated)")
  
```

## Heteroscedasticity 

### Some Heteroscedasticity in House Model (Sqft, Grade, Waterfront)

```{r}
#| code-line-numbers: "1|2-4|6"
library(rio);library(janitor);library(tidymodels)
DataHouses=import("https://ai.lange-analytics.com/data/HousingData.csv") |>
clean_names("upper_camel") |>
select(Price, Sqft=SqftLiving, Grade, Waterfront)

ModelHouses=lm(Price~., DataHouses)
```

## Heteroscedasticity 

### Some Heteroscedasticity in House Model (Sqft, Grade, Waterfront)

```{r}
#| code-fold: true
DataPlot=augment(ModelHouses) |> 
         mutate(Category=case_when(.fitted<500000~1,
                                  .fitted<1000000~2,
                                  .fitted<1500000~3,
                                  .fitted<2000000~4,
                                  .fitted<2500000~5,
                                  .fitted<3000000~6,
                                  TRUE~7)) |>
         group_by(Category) |>  
         mutate(TwoSDHigh=mean(.resid)+2*sd(.resid)) |> 
         mutate(TwoSDLow=mean(.resid)-2*sd(.resid))

ggplot(DataPlot)+
  geom_point(aes(x=.fitted, y=.resid, color="Residuals"), alpha=0.2)+
  geom_point(aes(x=.fitted, y=TwoSDLow, color="Mean - 2 SD"))+
  geom_point(aes(x=.fitted, y=TwoSDHigh, color="Mean + 2 SD"))+
  scale_color_manual(values = c("Residuals" = "black", "Mean - 2 SD" = "orange", "Mean + 2 SD" = "red")) +
  scale_x_continuous(limits = c(0,2500000))+
  scale_y_continuous(limits = c(-2000000,5000000))+
  labs(x = "Price (estimated)")
```

