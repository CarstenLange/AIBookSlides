---
title: "Linear Regression"
subtitle: "Univariate"
format: 
  revealjs:
    code-fold: true
    scrollable: true
    echo: true
    incremental: false
---



## What Will You Learn {.scrollable .smaller}

-   Reviewing the basic idea behind linear regression

-   Learning how how to measure predictive quality with Mean Square Error ($MSE$).

-   Understanding the role of parameters in a linear regression model (applies also to other models)

-   Calculating optimal regression parameters using OLS

-   Finding optimal regression parameters by trial and error

-   Finding optimal regression parameters with `lm` and `tidymodels`.

-   Distinguish between unfitted and fitted models

-   Using `tidymodels` to split observations randomly into a training and testing dataset.

-   Using `tidymodels` to analyze housing prices.

## First: A/B Test Revisited
### Data

-   [Wooldridge Dataset](https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf) (can be downloaded [here](https://econ.lange-analytics.com/RData/Datasets/DataWage.xlsx))
-   Wage data from 1976 for 526 female and male observations including variables such as Sex, Education, and Experience in years

```{r}
library(tidyverse)
library(wooldridge)

DataWage=wage1 |> 
         filter(smsa==1) |> 
         select(Wage=wage, SexFem=female)
head(DataWage)
```


## First: A/B Test Revisited
### t.test for equal means



```{r}
t.test(Wage~SexFem, data=DataWage, var.equal=TRUE)
```

```{r}
#| echo: false
library(tidymodels)
ResultsTTest=tidy(t.test(Wage~SexFem, data=DataWage, var.equal=TRUE))
cat("The estimated difference between female and male salary is:",
     ResultsTTest$estimate2-ResultsTTest$estimate1)

```


## First: A/B Test Revisited
### Linear Regression

```{r}
library(ggdag)
DAGSimpleRegression=dagify(Wage~SexFem, outcome="Wage",   
                           exposure="SexFem",  labels = c(Wage = "Wage", 
                                                  SexFem = "SexFem"))
ggdag_status(DAGSimpleRegression, use_labels = "label", 
             text = FALSE)
```
$$\widehat{Wage}=\beta_1 Sex_{Fem}+ \beta_2$$

## First: A/B Test Revisited
### Linear Regression

$$\widehat{Wage}=\beta_1 Sex_{Fem}+ \beta_2$$

```{r}
ModelLM=lm(Wage~SexFem, data=DataWage)
summary(ModelLM)
```

## First: A/B Test Revisited
### A/B Test and Simple Linear Regression (one variable) not suitable

```{r}
library(ggdag)
DAGSimpleRegression=dagify(Wage~SexFem+Exp+Educ+MSAYex, 
                           outcome="Wage",   
                           exposure="SexFem",  
  labels = c(Wage="Wage", SexFem="SexFem", Exp="Exp", Educ="Educ", MSAYex="MSAYex"))
set.seed(22)
ggdag_status(DAGSimpleRegression, use_labels = "label", 
             text = FALSE) 
```
## Jumping Right Into It with Real World Data

Univariate OLS (simple OLS) with a real estate dataset

**Data Description:**

King County House Sale dataset (Kaggle 2015). House sales prices from May 2014 to May 2015 for King County in Washington State.

**Simplication:**

- Several predictor variables, but for now we use only $Sqft$

- We will only use 100 randomly chosen observations from the total of 21,613 observations.

## Loading Libraries

```{r}
#| code-fold: false
library(tidymodels)
library(rio)
library(kableExtra)
library(janitor)
```

## Loading the Data 

```{r}
#| code-fold: false
DataHouses=
  import("https://ai.lange-analytics.com/data/HousingData.csv") |>
  clean_names("upper_camel") |>
  select(Price, Sqft=SqftLiving) 

set.seed(7771)
Data100Houses=sample_n(DataHouses,100)
```

**First six observations training data:**

```{r}
#| echo: false

head(Data100Houses)
```



## How much is a House Worth in King County?



A house with average properties should be predicted with an average price!


```{r}
MeanSqft=mean(Data100Houses$Sqft)
cat("The mean square footage of a house in King county is:", MeanSqft)

MeanPrice=mean(Data100Houses$Price)
cat("The mean price of a house in King county is:", MeanPrice)
```




## Predicting the Price of an Average Sized House as the Average of all House Prices

```{r}
#| echo: false
ggplot(aes(x=Sqft, y=Price), data=Data100Houses)+
  geom_point(color="red")+
  geom_hline(yintercept=mean(Data100Houses$Price))+
  geom_vline(xintercept=mean(Data100Houses$Sqft))+
  scale_y_continuous(limits=c(0,900000),  breaks = seq(0,900000,100000), labels = scales::comma)+
  scale_x_continuous(limits=c(0,5000),  breaks = seq(0,5000,500), labels = scales::comma)
```

## An Interactive Graph That Explains it All

[https://econ.lange-analytics.com/calcat/linregrmeans](https://econ.lange-analytics.com/calcat/linregrmeans){target="_blank"}

## From Unfitted to Fitted Model


How does the Unfitted Model Looks Like?

$$
\underbrace{\widehat{Price}}_\widehat{y}=\underbrace{\beta_1}_m \underbrace{Sqft}_x + \underbrace{\beta_0}_b
$$

Fitting the Model with lm()


```{r}
ModelOLS=lm(Price~Sqft, data=Data100Houses)
```


## Unfitted Model vs Fitted Workflow Model {.smaller}

Unfitted Model: $$
\underbrace{\widehat{Price}}_\widehat{y}=\underbrace{\beta_1}_m \underbrace{Sqft}_x + \underbrace{\beta_0}_b
$$


```{r}
print(ModelOLS)
```


Fitted Model: $$
\underbrace{\widehat{Price}}_\widehat{y}=\underbrace{240}_m \cdot\underbrace{Sqft}_x + \underbrace{52509}_b
$$

## Interpretation and Significance


```{r}
#| echo: false
tidy(ModelOLS)
```


$$
\begin{align}
\widehat{Price}&=240 \cdot Sqft +  52509\\
 (+240)&=240\cdot (+1) +  (+0)\\
 (+480)&=240\cdot (+2) +  (+0)\\
 (+720)&=240\cdot (+3) +  (+0)
\end{align}
$$ For each extra $Sqft$ the predicted price increases by $240

The variable $Sqft$ is significant. I.e., the probability that the related coefficient $\beta_1$ equals zero is extremely small.



## Univariate Linear Regression - Data Table and Goal {.smaller}

::: columns

::: {.column width="60%"}
**The Data Table**

```{r }
DataMockup=import("https://ai.lange-analytics.com/data/DataStudyTimeMockup.rds")
kbl(DataMockup |> mutate(i=1:5) |> select(i,everything()), 
    caption="Mockup Training Dataset")|>
  add_header_above(c(" ", "y", "x"), escape=F) |> 
  kable_styling(bootstrap_options=c("striped","hover"), full_width = F, position="center")
```
:::

::: {.column width="40%"}
**The Regression:**

$$
\widehat{y}_{i} = \beta_{1}x_{i}+\beta_{2}
$$

$$
\widehat{Grade}_{i} = \beta_{1}StudyTime_{i}+\beta_{2}
$$

**The Goal**

Find values for $\beta_1$ and $\beta_2$ that minimize the prediction errors $$(\widehat{y}_{i}-y_i)^2$$
$$(\widehat{Grade}_{i}-Grade_i)^2$$
:::


:::

## Predict Grade Based on Study Time with Eyeballing 


Copy the code from the code-section of the slide into an *R* script and try to adjust $\beta_1$ and $\beta_1$ to get the best prediction equation.

$$
\widehat{y}_{i} = \beta_{1}x_{i}+\beta_{2}
$$
$$
\widehat{Grade}_{i} = \beta_{1}StudyTime_{i}+\beta_{2}
$$

```{r}
#| eval: false
#| echo: true
#| code-fold: false
library(rio)
DataMockup=import("https://ai.lange-analytics.com/data/DataStudyTimeMockup.rds")
Beta1=0
Beta2=mean(DataMockup$Grade)- Beta1*mean(DataMockup$StudyTime)
cat("Beta2 ensuring avg. StudyTime leads to avg. Grade prediction:", Beta2)
source("https://ai.lange-analytics.com/source/RegrLinePlot4Slides.R")
```

## Univariate Linear Regression - Data Diagram and Goal {.smaller}

::: columns
::: {.column width="40%"}
**The Regression:**

$$
\widehat{y}_{i} = \beta_{1}x_{i}+\beta_{2}
$$
$$
\widehat{Grade}_{i} = \beta_{1}StudyTime_{i}+\beta_{2}
$$

**The Goal**

Find values for $\beta_0$ and $\beta_1$ that minimize the individual (squared) prediction errors: 
$$(\widehat{y}_{i}-y_i)^2$$
$$(\widehat{Grade}_{i}-Grade_i)^2$$
:::

::: {.column width="60%"}
**The Data Diagram**

```{r}
Model123=lm(Grade~StudyTime, data=DataMockup)
PredGrade=predict(Model123, DataMockup)
ggplot(DataMockup, aes(x=StudyTime,y=Grade)) +
  geom_line(aes(y=PredGrade), color="red", size=2.7)+
  geom_point(size=5, color="blue")+
  geom_point(aes(y=PredGrade), color="black", size=2.7)+
  geom_segment(aes(x = StudyTime, y = PredGrade,
                   xend = StudyTime, yend = Grade),size=1.2)+
  scale_x_continuous("Study Time", breaks=seq(1,8))+
  scale_y_continuous(limits=c(65,110), breaks=seq(60,100,5))
```

```{r}
#| eval: false
#| echo: true
Beta1=4
Beta2=64
```

:::
:::

# How to Measure Prediction Quality with the Mean Squared Error (MSE) {.smaller}

```{=tex}
\begin{eqnarray*}
MSE & = & \frac{1}{N} \sum_{i=1}^{N}(\widehat{y}_{i}-y_{i})^{2} \\
 & \Longleftrightarrow& \\
MSE & =  & \frac{1}{N} \sum_{i=1}^{N}(\underbrace{\overbrace{\beta_{1}x_{i}+\beta_2}^{\mbox{Prediction $i$}}-y_i}_{\mbox{Error $i$}})^2
\end{eqnarray*}
```
**Note, when the data are given (i.e.,** $x_i$ and $y_i$ are given), the $MSE$ depends only on the choice of $\beta_1$ and $\beta_2$ Â»

## How to Measure Prediction Quality with the MSE {.smaller .scrollable}

```{=tex}
\begin{eqnarray}
MSE & = & \frac{(\beta_1x_{1}+\beta_2-y_1)^2
       +(\beta_1x_{2}+\beta_2-y_2)^2 +  \cdots+
       (\beta_1x_{5}+\beta_2-y_{5})^2}{5} \nonumber \\
       & \Longleftrightarrow& \nonumber \\
MSE & = & \frac{1}{5}\left[ (\underbrace{\overbrace{\beta_1\cdot 2+\beta_2}^{\mbox{Prediction $1$}}-65}_{\mbox{Error $1$}})^2
       +(\underbrace{\overbrace{\beta_1\cdot 3+\beta_2}^{\mbox{Prediction $2$}}-82}_{\mbox{Error $2$}})^2\right.\nonumber \\
   &   &\nonumber \\
       && +
    (\underbrace{\overbrace{\beta_1\cdot 7+\beta_2}^{\mbox{Prediction $3$}}-93}_{\mbox{Error $3$}})^2 
+(\underbrace{\overbrace{\beta_1\cdot 8+\beta_2}^{\mbox{Prediction $4$}}-93}_{\mbox{Error $4$}})^2\nonumber \\
   &   &\nonumber \\
       && +\left. (\underbrace{\overbrace{\beta_1\cdot 4+\beta_2}^{\mbox{Prediction $5$}}-83}_{\mbox{Error $6$}})^2\right]
\end{eqnarray}
```


## Custom R Function to Calculate MSE {.scrollable}

```{r}
#| echo: false
FctMSE=function(VecBeta, data){
                        Beta1=VecBeta[1]
                        Beta2=VecBeta[2]
                        data=data |>
                        rename(Y=1, X=2) |> 
                        mutate(YPred=Beta1*X+Beta2) |>
                        mutate(Error=YPred-Y) |>
                        mutate(ErrorSqt=Error^2)
                        
                        MSE=mean(data$ErrorSqt)
                        
                        return(MSE)}
```

Function Call:

```{r}
#| echo: true
VecBeta=c(4,61)
ResultMSE=FctMSE(VecBeta, DataMockup)
print(ResultMSE)
```

Function Definition:Â»

```{r}
#| echo: true
FctMSE=function(VecBeta, data){
                        Beta1=VecBeta[1]
                        Beta2=VecBeta[2]
                        data=data |>
                        rename(Y=1, X=2) |> 
                        mutate(YPred=Beta1*X+Beta2) |>
                        mutate(Error=YPred-Y) |>
                        mutate(ErrorSqt=Error^2)
                        
                        MSE=mean(data$ErrorSqt)
                        
                        return(MSE)}
```

## How to Find Optimal Values for $\beta_1$ and $\beta_2$ {.smaller .scollable}

**Method 0: (not such a good idea)** 

:   We can use **unsystematic trial and error**.

    ```{r}
    #| echo: true
    library(rio)
    FctMSE=import("https://ai.lange-analytics.com/source/FctMSE.rds")
    VecBeta=c(7,60)
    ResultMSE=FctMSE(VecBeta, DataMockup)
    print(ResultMSE)
    ```


**Method 1:**

:   We can use a **systematic trial and error process**.

**Method 2:**

:   **Calculate optimal values** for the parameters (the $\beta s$) based on Ordinary Least Squares (OLS) using two formulas (**Note,** this method works only for linear regression)



## Method 1: Use a Systematic Trial and Error Process ğ¤

-   **Grid Search (aka Brute Force):**

    1.  For a given range of $\beta_1$ and $\beta_2$ values, build a table with pairs of all combinations of these $\beta s$.
    2.  Then use our custom `FctMSE()` command to calculate a $MSE$ for each $\beta$ pair.
    3.  Find the $\beta$ pair with the lowest $MSE$

-   **Optimizer:** Use the R build-in optimizer. Push the start values for $\beta_1$ and $\beta_2$ together with the data to the optimizer as arguments. The rest is done by the optimizer.

-   See the R script in the footnote to see both algorithms in action.Â»

::: footer
See: [LinRegrScript.R script100](https://econ.lange-analytics.com/RScripts/LinRegrScript.R)
:::

## Method 2: Calculate Optimal Parameters (only for OLS!){.smaller}

```{=tex}
\begin{eqnarray*} 
\beta_{1,opt}&=& \frac
{N \sum_{i=1}^N y_i x_i- \sum_{i=1}^N y_i \sum_{i=1}^N x_i} 
{N \sum_{i=1}^N x_i^2 - \left (\sum_{i=1}^N x_i \right )^2}=3.96\\
&& \nonumber \\
\beta_{2,opt.}&=& \frac{ \sum_{i=1}^N y_i  - \beta_1 \sum_{i=1}^N x_i} {N} = 64.18
\end{eqnarray*}
```

```{r}
#| echo: false
DataTable=DataMockup |>
  mutate(GradeXStudyTime=Grade*StudyTime) |>
  mutate(StudyTimeSquared=StudyTime^2) 


kbl(DataTable |> mutate(i=1:5) |> select(i,everything()), 
    caption="Mockup Training Dataset ")|>
  add_header_above(c(" ", "y", "x", "y x","x x"), escape=F) |> 
  kable_styling(bootstrap_options=c("striped","hover"), full_width = F, position="center")
```

```{r}
#| echo: false
kbl(data.frame(as.list(colSums(DataTable))), 
    caption="Column Sums")|>
    kable_styling(bootstrap_options=c("striped","hover"), full_width = F, position="center")


```


## Using lm() Command from Base R to Run a Regression {.scrollable}

```{r}
#| echo: true
ModelStudyTime=lm(Grade~StudyTime, data=DataMockup)
print(ModelStudyTime)
```

## Results from OLS Analysis and Sanity Check

**Unfitted Model**

$$Grade=\beta_1 \cdot StudyTime + \beta_2$$

**Fitted Model**

$$Grade=3.96  \cdot StudyTime + 64.18$$
Coefficient for $StudyTime$ ($\beta_1$) should be positive.

## Getting the Results from the OLS Model

```{r}
#| echo: true
#| code-fold: false
summary(ModelStudyTime)

```

## Predicting with the Fitted Model - Generated Data {.smaller}

$$Grade=3.96 \cdot StudyTime + 64.18$$
```{r}
#| echo: true
ModelStudyTime=lm(Grade~StudyTime, data=DataMockup)

DataForPrediction=data.frame(StudyTime=c(1,2,3))

predict(ModelStudyTime, newdata=DataForPrediction)
```

Interpretation of $\beta_1=3.96$:

1. From the formula: If $StudyTime$ increases, Grade increases. -> makes sense!

2. From the formula: Quantitative impact: If $StudyTime$ increases by one unit, Grade increases by $3.96$ units.

3. From the prediction: Above we predicted $Grade$ for $StudyTime$s of 1,2,3, which confirms point 1) and 2).

## Predicting with the Fitted Model - Training Data


$$Grade=3.96  \cdot StudyTime + 64.18$$
Generating the predictions:

```{r}
#| echo: true

ModelStudyTime=lm(Grade~StudyTime, data=DataMockup)

VecPred=predict(ModelStudyTime, newdata=DataMockup)
print(VecPred)
```

## Predicting with the Fitted Model - Training Data

$$Grade=3.96  \cdot StudyTime + 64.18$$
Augmenting the predictions to the data frame:

```{r}
#| echo: true
VecPred=predict(ModelStudyTime, newdata=DataMockup)
DataMockup=DataMockup |> 
           mutate(.pred=VecPred)
print(DataMockup)
```








# Real World Application

## Univariate OLS with a Real World Dataset

**Data Description:**

-   King County House Sale dataset (Kaggle 2015). House sales prices from May 2014 to May 2015 for King County in Washington State.
-   Several predictor variables. For now we use only $Sqft$
-   We will only use 500 randomly chosen observations from the total of 21,613 observations.

-We only use Sqft as predictor variable for now


## Univariate OLS with a Real World Dataset

**Data:**

```{r}
#| echo: true
library(rio)
DataHousing =
  import("https://ai.lange-analytics.com/data/HousingDataSmall.csv")
head(DataHousing)
```

## Univariate OLS with a Real World Dataset


**Unfitted Model:** 

$$\widehat{Price}=\beta_1 Sqft + \beta_2$$
**Directed Acyclic Graphs (causal graphs):**

```{r}
library(ggdag)
set.seed(123)
dagify(Price~Sqft,
       exposure= "Sqft",
       outcome = "Price") |> 
ggdag_status(node_size = 22)
```


## Univariate OLS with a Real World Dataset {.scrollable}

**Splitting in Training and Testing Datasets**

```{r}
#| echo: true
library(tidymodels)
set.seed(777)
Split7030=initial_split(0.7,data=DataHousing, strata = Price, breaks = 5)
DataTrain=training(Split7030)
DataTest=testing(Split7030)
```

**DataTrain**

```{r}
head(DataTrain)
```

**DataTest**

```{r}
head(DataTest)
```

## Univariate OLS with a Real World Dataset

**Run the Analysis with lm()**

```{r}
#| echo: true
library(rio)
DataHousing =
  import("https://ai.lange-analytics.com/data/HousingDataSmall.csv")
library(tidymodels)
Split7030=initial_split(0.7,data=DataHousing, strata = Price, breaks = 5)
DataTrain=training(Split7030)
DataTest=testing(Split7030)

ModelHouses=lm(Price ~ Sqft, data=DataTrain)
summary(ModelHouses)
```



## Predicting with tidymodels Training and Testing Data {.scrollable .smaller}

The augment() command first predicts and then augments the results to the related data frame

```{r}
#| echo: true
DataTrainWithPred=augment(ModelHouses, new_data = DataTrain)
DataTestWithPred=augment(ModelHouses, new_data = DataTest)

head(DataTrainWithPred)
head(DataTestWithPred)

```

## Metrics for Predictive Quality Training Data

```{r}
#| echo: true
metrics(DataTrainWithPred, truth = Price, estimate = .fitted)
```

## Metrics for Predictive Quality Testing Data

```{r}
#| echo: true
metrics(DataTestWithPred, truth = Price, estimate = .fitted)

```

## Summary - Steps for a Linear Regression {.smaller}


1. Draw a DAG (with R or by hand)

2. Mark if the effects in the DAG are expected to be positive or negative

3. Write down the formula for the unfitted model. Such as:
$$Price=\beta_1 Sqft + \beta_2$$

3. Run the regression

4. Substitute the $\beta s$ in the formula with the values you got from the regression.

5. Sanity test: Are the signs (positive/negative) what you expected. If not,  the related variable does not belong in your model

6. Interpret the $\beta (s)$. E.g., if predictor variable increases by one unit outcome increases by $\beta$ units. Note, the intercept coefficient cannot be interpreted in almost all cases!

7. Check if P value is low enough (e.g., smaller than 0.05=5%). This is the probability for the related coefficient to be 0 and thus irrelevant. You want a low probability for that event. If P value is too high the related variable does not belong in your model. Note, no need to interpret the P for the intercept. 

## Exercise

Now it is your turn. Pick a value from the housing data set and run a univariate regression to predict the price. 

```{r}
library(tidymodels)
library(rio)
library(janitor)
DataHouses=
  import("https://ai.lange-analytics.com/data/HousingData.csv") |>
  clean_names("upper_camel")

Split7030=initial_split(0.7,data=DataHouses, strata = Price, breaks = 5)
DataTrain=training(Split7030)
DataTest=testing(Split7030)
```

