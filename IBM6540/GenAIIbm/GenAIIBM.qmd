---
title: "Generative AI and Large Language Models"

format: 
  revealjs:
    code-fold: true
    scrollable: true
    echo: true
    incremental: false
---

## What Will You Learn/Review {.scrollable .smaller}

-   Why *Deep Learning* models have so many parameters

-   Understanding the Role of *Large Language Models* (*LLM*) in the *AI* Landscape

-   *Tokenization* of the English language

-   *One-Hot Encoding* of the English language

-   How an *LLM* guesses the next word?

-   How words can be embedded as vectors to code similar meanings (*Input Embedding*)<br> `Queen`, `King`, `Male`, `Female` but `Sand`

-   How the words position in a prompt can be embedded as vectors (*Positional Encoding*)<br> `Dog bites child` but `Child bites dog`

-   How attention of words in a prompt can be encoded (*Attention Mechanism*)<br> "The cat sits on the `mat` because `it` is soft"

-   How to create applications of *genAI*

-   How post-prepossessing can avoid discrimination, derogatory language, and racism

## Large Language Models in Context

![Source: Medium: <https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f>](images/clipboard-1934318519.png)

# Prelude: Simple Image Recognition with Neural Network {.smaller}

## Before we cover LLMs: Image Recognition (Cat, Dog, Bird) and One-Hot Encoding

### Size 224 x 224 and 3 colors = 150,528 Pixels

![Source: Medium https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](images/clipboard-2629028543.png)

## Before we cover LLMs: First Image Recognition (Cat, Dog, Bird)

### Size 224 x 224 and 3 colors = 150,528 Pixels

**Neural Network:** MLP with only two hidden layers (in reality: Convolutional Neural Network)

**Inputs:** 150,528

**Outputs:** 3

**Hidden Neurons:** E.g. 150,528

**Parameters:** 150,528 \**150,528+ 150,528+*150,528 \**150,528+ 150,528 + 150,528*4 + 3=45,318,110,209

## Before we cover LLMs: First Image Recognition (Cat, Dog, Bird)

### Size 224 x 224 and 3 colors = 150,528 Pixels

![Source: Mediumhttps://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](images/clipboard-1198050597.png)

# Large Language Models and GenAI

## Step 0a: Tokenization

1.  Tokenization: The words (and subwords) of a language are simplified. Each (sub)word becomes a token. E.g.:

    "An example: untrained horses eat apples, but apples do not eat untrained horses"

![](images/clipboard-3146099741.png){width="696"}

For more examples like this, see: [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer){target="_blank"}

## Step 0b: One-Hot Encoding {.columns}

::: {.column width="80%"}
### Procedure

A vector with the length of the tokens for the language is created (e.g., for English, about 200,000 words/tokens)

The value for all words/tokens is zero with the exception of the word/token of interest (One-Hot)
:::

::: {.column width="20%"}
### Example:

![](images/clipboard-1984520822.png)
:::

## Ridiculous Simple Example - untrained (Guess the Next Word) {auto-animate="true"}

![](images/clipboard-2369666718.png)

## Ridiculous Simple Example - untrained (Guess the Next Word) {auto-animate="true"}

![](images/clipboard-2369666718.png){width="166" height="106"}

## Ridiculous Simple Example - trained (Guess the Next Word) {auto-animate="true"}

![](images/clipboard-2592421233.png)

## Problems with the Simple Example {.columns .smaller}

::: {.column width="80%"}
### Problems and Solutions

1.  On-Hot Vectors have **no semantics**. E.g., *Queen* and *King* should be encoded in a similar way, but with *One-Hot*, they are not. Idealy, the resulting vectors should be similar (possible measures: *Euclidean Distance* or *Dot Product*)<br> **Solution:** `Word Embedding`. Helps with semantic interpretation.
2.  In contrast to *Recurrent Neural Networks*, *Transformers* read the whole prompt at once and therefore do not maintain order.<br> **Solution:** `Positional Encoding`
3.  **Attention is ignored.** <br>E.g.: <br> `The animal didn't cross the road because it is too tired`<br> or <br> `The animal didn't cross the road because it is too wide`<br> **Solution:** `Attention` (stores for each word which words from the prompt are important and which are not)
:::

::: {.column width="20%"}
### Simple Model:

![](images/clipboard-2592421233.png)
:::

## Source

The following is based on:

-   [Super Data Science Course: GenAI & LLMs A-Z](https://community.superdatascience.com/c/llm-gpt)<br>

The course is not free, but a free trial exist. The 4 hour course is **strongly recommended** to dive deeper.

## History

**2003:** Bengio, Y., Neural Probabilistic Language Models (very basic)

**2014:** Sutskever, I.: Recurrent Neural Networks - LSTM (mainly used for translation; slow and not easy to parallelize)

**2014:** Bahdanu, D.: Still Recurrent Neural Networks but with Attention - LSTM (still slow and not easy to parallelize, but more powerful)

**2017:** Google Team, "Attention is All you Need", Transformer Model (powerful, fast, and can be parallelized; with small modifications still used in 2025)

[Source: Andre Karpathy, http://youtube.com/watch?v=XfpMkf4rD6E]{style="font-size:70%"}

## Overview LLMs {.smaller}

![](images/clipboard-340487702.png)

Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=12](https://online.fliphtml5.com/grdgl/qhfw/#p=12){target="Blank_"}

## Step 1: Word Embedding to Provide Sentiment

`King + Female - Male = ???`

. . .

`King + Female - Male = Queen`

. . .

<small>Similar words should have similar encoding vectors:<br><br> `great` should be similar to `amazing`. The input vector for `tomato` should be similar to the one for `apple`, and the input vectors for `queen` and `king` should also be similar.\
Similarity can be measured by *Euclidean Distance* or similar measures (LLMs often use the *Dot-Product*).</small>

[This and the explanation of the following steps is based on the *Super Data Science* course [GenAI & LLMs A-Z](https://community.superdatascience.com/c/llm-gpt "GenAI Course from Super DataDataScience"){target="_blank"}]{style="font-size:42%"}

## (Step 1): Hire a Linguist to do the Word Embedding

The linguist performs *Word Embedding* into 512 categories of their choice.

This method is prohibitively slow but bear with me.

## (Step 1): Hire a Linguist - Possible Word Embedding {.smaller}

Below are possible *Word Embeddings* for `apple`, `tomato`, `queen`, `king`, `amazing`, `great`

![](images/clipboard-1636635893.png)

## Step 1: Use a Neural Network - Possible Word Embedding {.smaller}

Below are possible *Word Embeddings* for `apple`, `tomato`, `queen`, `king`, `amazing`, `great`

![](images/clipboard-3233943967.png)

## Step 1: Word Embedding with Bag of Words {.smaller}

### (Window Size = 3 --- Amazing, Great, and Delisious are coded similary)

![](images/clipboard-1212096732.png)

## Step 1: Word Embedding Real World Example {.smaller}

<https://colab.research.google.com/drive/10AiswVTzzgr7dlJJEe_t3Pgm3evbGtzP#scrollTo=-chXmtjNVNbz>

## Step 2: Finding the Correct Positioning for Words

**Recurrent Neural Networks** (predecessor of *Transformers*), processed the prompt word by word and had a memory of previous processed words. Thus they **maintained the order of the words in the prompt**. Unfortunately, this took a long time (note, prompts of 32,000 tokens are common now).

**Transformers** process all tokens simultaneously, thus **losing the order of the words in the prompt**.

**For more details:** Paleti, Nikhil Chowdary. “Positional Encoding Explained: A Deep Dive into Transformer PE.” The Deep Hub (blog), July 12, 2024. [https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b){target="_blank"}

## Step 2: Generate Position-Aware Embedding {.smaller}

Add a *Position-Vector* (512 x 1) to each word embedding (512 x 1).

The *Position-Vector* reflects the word position in the prompt (prompt size = 32,000 in ChatGPT).

![](images/clipboard-1514739384.png)

## Step 2: Generate Position-Aware Embedding

### Example Specifications

**Prompt Size is 301:** Possible positions 0 to 300 (Prompt Size: 32,000 in *ChatGPT*)

**Position-Vector Size is 17:** Dimensions 0 to 16 (512 dimensions in *ChatGPT*)

## Step 2: Generate Position-Aware Embedding {.smaller .columns}

::: {.column width="65%"}
#### Example (not what GPT uses): Position Table with 1, 10, 100 Digits

```{r}
#| echo: false
library(tidyverse)
library(kableExtra)
TablePos=data.frame(WordPosition = 0:300)

# Extract digits
TablePos$Hundreds  <- (TablePos$WordPosition %% 1000) %/% 100
TablePos$Tens   <- (TablePos$WordPosition %% 100) %/% 10
TablePos$Ones    <- TablePos$WordPosition %% 10

# View the first few rows
kbl(TablePos)

```
:::

::: {.column width="30%"}
#### Problems

-   For efficient trainig in Neural Networks numbers should be between 1 and -1

-   For the *Position Vector* 17 (0 to 16) dimensions are needed (512 for *ChatGPT*). Here, the positional encoding produces only 3 dimensions.

#### Note:

The three digits (dimensions) have repeating cycles with different frequencies. This is how we encode numbers.
:::

::: {.column width="5%"}
:::

## Step 2: Generate Position-Aware Embedding

### Frequency of Digits for 1, 10, 100 Digits (not what GPT uses)

```{r}
#| echo: false
Plot=ggplot(aes(x=WordPosition, y=Ones), linesize=9, data=filter(TablePos, WordPosition<300))+
  geom_line(aes(x=WordPosition, y=Ones), color="blue", size=0.7)+
  geom_line(aes(x=WordPosition, y=Tens), color="red", size=1.2)+
  geom_line(aes(x=WordPosition, y=Hundreds), color="magenta", size=1.8)+
  scale_x_continuous(breaks = seq(0,300,10))+
  scale_y_continuous(breaks = seq(0,10,1))+
  labs(title = "Digits One (blue), Ten (red), and Hundred (magenta)",
       x = "Word Position",
       y = "Value") 
Plot

```

## Step 2: Generate Position-Aware Embedding

### Sinusoidal Positional Encoding in Transformers - Example

$$
PE_{(pos, i)} = 
\begin{cases}
\sin\left( \frac{Pos}{10000^{i/\text{Dim}}} \right) & \text{if } i \text{ is even} \\
\cos\left( \frac{Pos}{10000^{(i-1)/\text{Dim}}} \right) & \text{if } i \text{ is odd}
\end{cases}
$$ $Pos :=$ The position of the word/token in the prompt. Numbered from $0$ to $300$ (32,000 in *ChatGPT*)

$i :=$ The row (dimension) in the *Position Vector* numbered from $0$ to $16$ (512 in *ChatGPT*)

$Dim :=$ The number of dimension (*length*) in the *Position Vector*. Here: $17$ ($512$ in *ChatGPT*)

## Step 2: Generate Position-Aware Embedding {.smaller}

### Sinusoidal Positional Encoding in Transformers - Example

```{r}
#| echo: false
# Set parameters
positions <- 301
d_model <- 17

# Initialize a matrix for positional encodings
PE <- matrix(0, nrow = positions, ncol = d_model)

# Compute positional encodings
for (pos in c(0:(positions-1))) {
  for (i in c(0:(d_model-1))) {
    
    if (i %% 2 == 0) {
      angle_rate <- pos / (10000 ^ (i / d_model))
      PE[pos+1,i+1] <- sin(angle_rate)
    } else {
      angle_rate <- pos / (10000 ^ ((i-1) / d_model))
      PE[pos+1,i+1] <- cos(angle_rate)
    }
  }
}

# Create a data frame with positions and PE values
PE_df <- data.frame(position = c(0:(positions-1)), PE)
ColTitles= c("WordPosition", paste0("X", 0:(d_model-1)))
colnames(PE_df)=ColTitles
# View first few rows
#kbl(PE_df)

```

```{r}
#| echo: false
library(knitr)

kable(PE_df, format = "html") %>%
  kable_styling(full_width = TRUE) %>%
  scroll_box(width = "100%", height = "500px", box_css = "overflow-x: auto;")

```

## Step 2: Generate Position-Aware Embedding

### Frequency of Digits

```{r}
#| echo: false
Plot=ggplot(aes(x=WordPosition, y=X2), size=9, data=PE_df)+
  geom_line(aes(x=WordPosition, y=X2), color="blue", size=0.7)+
  geom_line(aes(x=WordPosition, y=X5), color="red", size=1.2)+
  geom_line(aes(x=WordPosition, y=X8), color="magenta", size=1.8)+
  geom_line(aes(x=WordPosition, y=X16), color="pink", size=1.8)+
  scale_x_continuous(breaks = seq(0,300,10))+
  scale_y_continuous(breaks = seq(-1,1,0.2))+
  labs(title = "Digits X2 (blue), X5 (red), X8 (magenta), and X16 (pink)",
       x = "Word Position",
       y = "Value") +
  geom_vline(xintercept = 100, size=0.8) +
  annotate("text", x = 100, y = max(0.9), label = "Pos=100", 
           angle = 90, vjust = -0.5, color = "black")
Plot

```

## Step 3: Adding Attention

**Example 1:**<br>The **animal** didn’t cross the road because **it** was too **tired**.

**Example 2:**<br>The animal didn’t cross the **road** because **it** was too **wide**.

Step 3 identifies for each word in the prompt other words in the prompt that are relevant (**Attention**)

Simplified:

**In Example 1** the word **it** points to **animal**.

**In Example 2** the word **it** points to **road**.

## Step 3: Adding Attention {.smaller}

Using `Position-Aware` `Vector X` and a  *linear Neural Network* ($Act_i=Inp^{eff}_i$) to create new vectors `Q-Vector` (what the word searches for), `K-Vector` (what the word has to offer),`V-Vector` (content of Vector X slightly modified).

![](images/clipboard-2021075770.png)

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=72](https://online.fliphtml5.com/grdgl/qhfw/#p=72){target="_blank"}</small>

## Step 3: Adding Attention {.smaller}

Comparing a words `Q-Vector` (what it wants) with all other words `K-Vectors`, including its own, using the **Dot Product**. The results are processed via *SoftMax* for all words and are written in an *importance vector* (see the yellow framed vector). It shows the importance of each other word in the prompt for the analyzed word (`it`).

![](images/clipboard-3133032979.png)

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=73](https://online.fliphtml5.com/grdgl/qhfw/#p=73){target="Blank_"}</small>

## Step 3: Adding Attention {.smaller}

The final *Position* and *Attention-Aware* `Vector A` for the word of interest (`it`) is created as a weighted sum from all `V-Vectors` in the prompt.

The procedure described above is applied to all words in the prompt.

![](images/clipboard-3133032979.png)

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=73](https://online.fliphtml5.com/grdgl/qhfw/#p=73){target="Blank_"}</small>

## Step 4: Multi Head Processing {.smaller}

**Multi Head Processing:** When creating `Q`, `K` and `V` vectors, they are created with a smaller dimensionality. Here 8 vectors with dimension 64 each.

This allows *Self-Attention* to use different focus for each section.

![](images/clipboard-3812760553.png)

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=75](https://online.fliphtml5.com/grdgl/qhfw/#p=75){target="_Blank"}</small>

## Step 4: Multi Head Processing (Visualization from Google)

[https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC){target="\"Blank_"}

## Steps 5a, 5b, 5c, 5d {.columns .smaller}

::: {.column width="60%"}
**Residual Connection:** Add information from previous steps (again) to the system. In case it got lost during training.

**Normalization** of *attention-aware* vector

**Feed Forward Neural Network (MLP)** to add non-linearity

**Multiple Layers with Self Attention** (96 in *GPT3*)
:::

::: {.column width="40%"}

![](images/clipboard-2833345460.png) 

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=12](https://online.fliphtml5.com/grdgl/qhfw/#p=12){target="Blank_"}</small>

:::

## Step 6: Training {.smaller}

All words (*position* and *attention-aware vectors*) - `A-Vectors` - from the prompt are simultaneously moved through a non-linear model, and the next word for each word in the prompt is predicted.

![](images/clipboard-480391752.png)

<small>Source: [https://online.fliphtml5.com/grdgl/qhfw/#p=130](https://online.fliphtml5.com/grdgl/qhfw/#p=130){target="_blank"}</small>

## Parallelization

Most tasks such as *Content-Awareness* and *Position Awareness* are processed for individual words/tokens. This means they can be assigned to different cores and computer clusters. Making training very fast as a result of **Parallelization**

Not only the next word of a prompt is predicted but also the next words for all other words in the prompts. That seems like a waste as we do know the words. It is a waste in the production state of the transformer, but in the training state (by masking each word's following word), the GPT can learn the next word for the complete *Context Window*. **A Transformer can learn 32,000 words in one run**.

## LLM (Fine) Tuning {.smaller}

After a model is trained with *language* (i.e., the complete Internet), it needs to be *fine-tuned* to perform specific tasks:

**Self-supervised:** Train on the desired data from the beginning (extremely expensive)

**Reinforcement Learning from Human Feedback (RLHF):**

1) Work with labeled data from humans
2) Create reward model
3) Adjust original model

**Supervised Learning**

## Supervised Learning {.smaller}

**Transfer Learning:** 

- Freeze all parameters except the last one or two layers (could also be different layers)
- retrain last  one or two layers with new data

**Parameter Efficient Fine Tuning (PEFT) Methods**

1) **Adapters:** Freeze parameters. Then add adapters to the model, and train the parameters of the *adapters* with the new data
2) **Low-Rank Adaptation:** Generate for each parameter matrix in the model, two matrices that when multiplied give the values of the matrix (*decomposing* $A$). E.g., $A$ has the dimension $512\times 512$ and $B$ has $512\times2$ and $C$ has $2\times512$. So that $B\cdot C=A$. Freeze $A$ and train $B$ and $C$ with new data (these are 2048 elements rather than 262144 elements). Multiply $B$ and $C$ and add it to $A$. This gives the *fine-tuned* weights.



3) **Prompt Tuning:** E.g. *RAGs*






## Where to Learn More About Transformers and LLM

**GenAI & LLMs A-Z (Advanced Technical Course)** <br> A Super Data Science course. The course is not free but a free trial of Super DataScience is available.

[https://community.superdatascience.com/c/llm-gpt](https://community.superdatascience.com/c/llm-gpt){target="blank_"}

## RAGs - Prompt Engineering in Combination with Search Engine

![Source: Medium https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](images/clipboard-1202208612.png)

## Prompt Engineering (Example: Gift Recommender)

[https://ai.centillionware.com/gift/](https://ai.centillionware.com/gift/){target="blank_"}

## Prompt Engineering (Example: Gift Recommender)

```         
$prompt = "
```

```         
    Find a birthday gift that cost between $budget_min dollars and $budget_max for a $sex person who is $age years old. 
```

```         
    The person's hobbies are $hobbies.
```

```         
    Sport is $sport_scale. $sport
```

```         
    Movies are $movie_genre_scale. $movie_genre
```

```         
    Books are $author_scale. $author
```

```         
    Food is $food_scale. $food
```

```         
  ";
```

## Post Processing: Bias, Discrimination, Derogatory Language

Example: Pi chat bot
