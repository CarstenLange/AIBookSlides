---
title: "Interactive Exercises for k-Nearest Neighbors: Predict Wine Color"
format: live-html


webr:
  packages: ['dplyr', 'rsample', 'janitor',  'recipes', 
             'yardstick','parsnip','workflows', 'kknn'] # Auto-install packages in the browser

engine: knitr
---


{{< include .././_extensions/r-wasm/live/_knitr.qmd >}}
{{< include .././_extensions/r-wasm/live/_gradethis.qmd >}}

<span style="color: red; font-weight: bold;">Your browser is loading R and all needed packages. That can take a while (about 1:00 - 2:00 minutes). Please be patient. You can see the progress in the right lower corner of your browser.</span>

In what follows, you will develop your own *k-Nearest Neighbors* model to predict the color of wines based on chemical properties. In contrast to the previous section, you will extend the analysis to use all chemical properties available in the wine dataset.

Below you can see all variables available in the wine dataset. The first variable $WineColor$ is the outcome variable, followed by the predictor variables that indicate the chemical properties of a wine. The  variable $Quality$ which reflect how consumers rated the quality of the wines and which is in the original dataset, is omitted. This variable is not a chemical property and is likely irrelevant for predicting a wine's color. 
```{webr}
#| label: ex_all
#| include: false

set.seed(876)

DataWine = read.csv("https:///econ.lange-analytics.com/RData/Datasets/WineData.csv") |> 
           clean_names("upper_camel") |> 
           select(-Quality) |> 
           rename(Sulfur=TotalSulfurDioxide) |> 
           mutate(WineColor=as.factor(WineColor))

Split7030=initial_split(DataWine, prop=0.7, strata=WineColor)

DataTrain=training(Split7030)

DataTest=testing(Split7030) 

RecipeWine=recipe(WineColor~., data=DataTrain) |>
           step_naomit() |> 
           step_normalize(all_predictors()) 

ModelDesignKNN=nearest_neighbor(neighbors=4, weight_func="rectangular") |>
               set_engine("kknn") |> 
               set_mode("classification")

WFModelWine=workflow() |> 
            add_recipe(RecipeWine) |>
            add_model(ModelDesignKNN) |> 
            fit(DataTrain)

DataTestWithPred=augment(WFModelWine, DataTest)
```

```{webr}
#| exercise: ex_0
#| exercise.setup: ex_all
#| autorun: true
#| runbutton: false
head(DataWine)
```

## Exercise 1 

The code block below uses the already loaded wine dataset (`DataWine`). The variable $WineColor$ was coded as a `factor` data type as required by the *k-Nearest Neighbors* algorithm.

After the random number generator has been initialized, the data are split into training and testing data (see Section \@ref(KNearNeigh-Data) for details).

**Now it is your turn:** Please complete the two commands that extract training and testing data from the split to assign them to the data frames `DataTrain` and `DataTest`.

```{webr}
#| exercise: ex_1
#| exercise.setup: ex_all
set.seed(876)
Split7030=initial_split(DataWine, prop=0.7, strata=WineColor)

DataTrain=______(______)
DataTest=______(______)
```

::: { .hint exercise="ex_1"}
**Here is a hint:**

The command to extract the *training data* from the initial split is called `training()`. 
The one for the *testing data* is called `testing()`.

The initial split is stored in the R object `Split7030`. 
The latter includes the data frame `DataWine` together 
with an indicator for each observation, if it 
belongs to the *training data* or the `testing data`.

`Split7030` is the only argument needed in the commands `training()` and `testing()`.
:::

::: {.solution exercise="ex_1"}
**Here is the solution:**

```{webr}
#| exercise: ex_1
#| solution: true
set.seed(876)
Split7030=initial_split(DataWine, prop=0.7, strata=WineColor)

DataTrain=training(Split7030) #<1>
DataTest=testing(Split7030)  #<2>
```                                

1. The commands `training()`  generates   the  *training data*. `Split7030` determines which observations from `DataWine` are sorted into the *training data*
2. The commands `testing()`  generates   the  *testing data*. `Split7030` determines which observations from `DataWine` are sorted into the *testing data*
:::

```{webr}
#| exercise: ex_1
#| check: true
gradethis::grade_this_code()
```

## Exercise 2 

This exercise requires some preparation. By **executing** the code block below **without editing**, you will create the **recipe** and the results will be saved into the *R* object `RecipeWine`. The command `tidy` outputs the recipe as a data frame that is easy to read:

```{webr}
#| exercise: ex_2prep1
#| exercise.setup: ex_all
RecipeWine=recipe(WineColor~., data=DataTrain) |> 
           step_naomit() |> 
           step_normalize(all_predictors())
tidy(RecipeWine)
```

You also need to define the **model-design**. When you execute the commands below **without editing** the *model-design*  will be created and saved into the *R* object `ModelDesignKNN`:

```{webr}
#| exercise: ex_2prep2
#| exercise.setup: ex_all
ModelDesignKNN=nearest_neighbor(neighbors=4, 
                                weight_func="rectangular") |>
               set_engine("kknn") |> 
               set_mode("classification")

ModelDesignKNN
```

**Now you will work with code:** You are  tasked with adding the *recipe* and the *model-design* to a **workflow** and then fitting the workflow to the training data. Note that the *recipe* `RecipeWine`, the *model-design* `ModelDesignKNN`, and the data frame `DataTrain` have already been loaded in the background. Please complete the code block below and execute it.

```{webr}
#| exercise: ex_2
#| exercise.setup: ex_all
WFModelWine=workflow() |> 
            add_recipe(______) |>
            add_model(______) |> 
            fit(______)

WFModelWine
```

::: {.hint exercise="ex_2"}
**Here is a hint:**

The *recipe* (`RecipeWine`) and the *model-design* (`ModelDesignKNN`) from the previous exercises   have been loaded in the background. They  need to be **added** to the *workflow*.

To **fit the workflow** and make it a *fitted workflow model*, we
use the `fit()` command. We always use the *training data* to *fit* a model (in this case  `DataTrain`)
:::

::: {.solution exercise="ex_2"}
**Here is the solution:**

```{webr}
#| exercise: ex_2
#| solution: true
WFModelWine=workflow() |> 
            add_recipe(RecipeWine) |>
            add_model(ModelDesignKNN) |> 
            fit(DataTrain)

WFModelWine
```
:::

```{webr}
#| exercise: ex_2
#| check: true
gradethis::grade_this_code()
```

## Exercise 3

Because the workflow model is fitted to the training data, we can use it for predictions. Instead of using the `predict()` command, we  use the more comprehensive `augment()` command, which predicts $WineColor$ based on the predictor variables from the testing dataset and then adds the predictions as a new variable named $.pred\_class$ to `DataTest`. The complete result is saved in the *R* object `DataTestWithPred`.

Since the *R* object `DataTestWithPred` contains the predictions (`estimate`) for the wine color and also the `truth`, stored in variable $WineColor$, we can use it as the data argument for metrics commands. An example is the `conf_mat()` command that generates a confusion matrix. The related code is already prepared. You just have to execute it.

```{webr}
#| exercise: ex_3
#| exercise.setup: ex_all
DataTestWithPred=augment(______, DataTest)   ______
conf_mat(DataTestWithPred, truth=WineColor, estimate=______)
```

::: { .hint exercise="ex_3"}
**Here is a hint:**

The `augment()` command generates the predictions based on the *testing dataset* and *augments* the *testing dataset* with a new column `.pred_class` which hold the predictions.

To create  the confusion matrix you have to provide the column with the predictions `.pred_class` which will e compared to the true wine color (`Winecolor1`)
:::

::: {.solution exercise="ex_3"}
**Here is the solution:**

```{webr}
#| exercise: ex_3
#| solution: true
DataTestWithPred=augment(WFModelWine, DataTest)   
conf_mat(DataTestWithPred, truth=WineColor, estimate=.pred_class)
```
:::

```{webr}
#| exercise: ex_3
#| check: true
gradethis::grade_this_code()
```

## Exercise 4


A first glance at the diagonal elements of the confusion matrix above already indicates an improvement over the model with two predictor variables.

The `tidymodels` package can help you to calculate metrics such as *accuracy*, *sensitivity*, and *specificity*. Instead of calculating these metrics separately you can streamline the process by creating a *metric set* first.\index{metric\_set()}

In the code block below, the command `metric_set()` creates a new command that we name `MetricsWine` (you can choose a different name if you like).

The newly created command `MetricWine()` can be used similarly to the `conf_mat()` command, but instead of creating a confusion matrix, it creates all the metrics previously specified with `metric_set()` simultaneously. This saves you some typing effort.

Give it a try with the code block below (note, the data frame `DataTest` and the fitted *workflow model* `WFModelWine` have already been loaded in the background):

```{webr}
#| exercise: ex_4
#| exercise.setup: ex_all
MetricsWine=metric_set(accuracy, sensitivity, specificity)
MetricsWine(DataTestWithPred, truth=______, estimate=______)
```

::: { .hint exercise="ex_4"}
**Here is a hint:**
The command 'MetricsWine()' that was generated in the first line of code requires like the confusion matrix command three arguments:

1. The data frame that contains the predictions as well ea the true values.
2. The column in that dataset that contains the true values.
2. The column in that dataset that contains the true predictions.
:::

::: {.solution exercise="ex_4"}
**Here is the solution:**

```{webr}
#| exercise: ex_4
#| solution: true
MetricsWine=metric_set(accuracy, sensitivity, specificity) #<1>
MetricsWine(DataTestWithPred, truth=WineColor, estimate=.pred_class) #<2>
```

1. Creates a new command that simultaneously calculates *accuracy*, *sensitivity*, and *specificity*.
2. Provides the data frame that contains the predictions as well as the true values.
:::


As the confusion matrix and the three metrics above confirm, we reached an almost perfect prediction quality.

Try changing the arguments in the `metric_set()` command to different metrics and see what happens.

```{webr}
#| exercise: ex_4
#| check: true
gradethis::grade_this_code()
```
