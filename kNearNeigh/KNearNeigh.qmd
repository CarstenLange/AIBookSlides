---
title: "k Nearest Neighbors"
subtitle: "These slides are interactive and must load a full version of R plus some packages. Please wait until complete (see the  lower right corner of the screen for progress"
author:
  name: "Carsten Lange"
  email: "clange@cpp.edu"
  affiliation: "Cal Poly, Pomona"

format: 
  live-revealjs:
    theme: [moon,CustomCL.scss]
    smaller: true
    controls: true
    chalkboard:
      theme: whiteboard
      boardmarker-width: 2.7
    incremental: false
    scrollable: true
    footer: "<a href='https://ai.lange-analytics.com/htmlbook/KNearNeigh.html' target='_blank'>Textbook </a> "
   
  
webr:
  packages: ['dplyr', 'rsample', 'janitor',  'recipes', 
             'yardstick','parsnip','workflows', 'kknn',
              'kableExtra'] # Auto-install packages in the browser

    
  
 

engine: knitr
---

{{< include _extensions/r-wasm/live/_knitr.qmd >}}


## Before we Begin Let Us Do A Thought Experiment

```{webr}
#| autorun: true
#| echo: false
DataRoomMates <- data.frame(
  name = c("Student 1", "Student 2", "Student 3"),
  Tidy = c(1, 8, 3),   # 1 = messy, 10 = very tidy
  Social = c(7, 2,5 )     # 1 = quiet, 10 = very social
)

DataWine=readRDS(url("https://ai.lange-analytics.com/data/WineData.rds")) |> 
         clean_names("upper_camel") |> 
         select(WineColor,Sulfur=TotalSulfurDioxide,Acidity) |> 
         mutate(WineColor=as.factor(WineColor))

set.seed(876)
Split7030=initial_split(DataWine,prop=0.7,strata = WineColor)
DataTrain=training(Split7030)
DataTest=testing(Split7030)

RecipeWine=recipe(WineColor~., data = DataTrain) |>
            step_naomit() |> 
            step_normalize(all_predictors()) 

ModelDesignKNN=nearest_neighbor(neighbors = 4, weight_func = "rectangular") |>
               set_engine("kknn") |> 
               set_mode("classification")

WFModelWine=workflow() |> 
            add_recipe(RecipeWine) |>
            add_model(ModelDesignKNN) |> 
            fit(DataTrain)

DataPredWithTestData=augment(WFModelWine, DataTest)
```

**I need three volunteers. Everybody else, please follow along on one computer in groups of three.**

Imagine the three of you are moving into a shared apartment.

There are two bedrooms: two people will share a big room, and one person will have a small room alone.

**To minimize conflicts, we want the two most similar people to share the two rooms.**

Please rate yourselves on a **scale from 1 to 10** for the two criteria below:

1. How tidy you are (**1=messy, 10= tidy**)

2. How social you are at home (**1=quiet, 10=very social**)

**We will treat your answers as data points and use Euclidean distance to decide who should share.**

##  Thought Experiment: Data Entry

For `Student1`, `Student2`, and `Student3`, let us enter the names of the three volunteer student. Then we replace the sample scores (e.g., `Tidy = c(1, 8, 3)`) with the scores from the volunteer students (in the correct order of `Student1`, `Student2`, and `Student3`).

Afterwards, click <span style="background-color: #75AADB; padding: 3px; color: white;">Run Code</span> (note, you need to run this code in order to work with the following examples).
```{webr}
#| warning: false
#| message: false
library(kableExtra)
DataRoomMates <- data.frame(
  name = c("Student 1", "Student 2", "Student 3"),
  Tidy = c(1, 8, 3),   # 1 = messy, 10 = very tidy
  Social = c(7, 2,5 )     # 1 = quiet, 10 = very social
)
kable(DataRoomMates)
```

## Thought Experiment: Plotting the Data

```{webr}
library(ggplot2)
ggplot(DataRoomMates, aes(x = Tidy, y = Social, label = name)) +
  geom_point(size = 4) +
  geom_text(vjust = -1) +
  scale_x_continuous(limits = c(0.5, 10.5), breaks=seq(1:10)) +
  scale_y_continuous(limits = c(0.5, 10.5), breaks=seq(1:10)) +
  coord_fixed(ratio = 1) +   # <-- makes the plot square
  labs(
    title = "Roommate Matching Using Euclidean Distance",
    x = "Tidy (1 = messy, 10 = tidy)",
    y = "Social at Home (1 = quiet, 10 = social)"
  ) 
```

$$EucDist^2=DiffTidy^2+DiffSocial^2$$

$$EucDist=sqrt(DiffTidy^2+DiffSocial^2)$$

## Compute Euclidean distances


```{webr}
MatrixDistance <- dist(DataRoomMates[, c("Tidy", "Social")], method = "euclidean")
print(as.matrix(MatrixDistance))
```

**Testing the distance between the first and the second student:**
$$EucDist^2=DiffTidy^2+DiffSocial^2$$

$$EucDist=sqrt(DiffTidy^2+DiffSocial^2)$$

```{webr}
DiffTidy=DataRoomMates[[1,2]]-DataRoomMates[[2,2]]
cat("The difference in Tidy between the first and the second student is:",DiffTidy)
DiffSocial=DataRoomMates[[1,3]]-DataRoomMates[[2,3]]
cat("The difference in Social between the first and the second student is:",DiffSocial)
EucDist=sqrt((DiffTidy^2+DiffSocial^2))
cat("Euclidean Distance=",EucDist)
```

## Use the Same Scale for All Predictor Variables

#### It is very important that all predictor variables have the same scale

Like here, both variables are in a range from 1 to 10.

#### What happens if we scale `Social` from 10 to 100 instead from 1 to 10

```{webr}
DataRoomMatesScaled=DataRoomMates |> 
                    mutate(Social=Social*10)

kable(DataRoomMatesScaled)
```

## The Influence of Social Compared to Tidy becomes very Small

```{webr}
#| fig-width: 1.7
#| fig-height: 17
library(ggplot2)
ggplot(DataRoomMatesScaled, aes(x = Tidy, y = Social, label = name)) +
  geom_point(size = 4) +
  geom_text(vjust = -1) +
  scale_x_continuous(limits = c(1, 10), breaks=seq(1,10,9)) +
  scale_y_continuous(limits = c(10, 100), breaks=seq(10,100,10)) +
  coord_fixed(ratio = 1) +   # <-- makes the plot square
  labs(
    title = "Roommate Matching Using Euclidean Distance",
    x = "Tidy (1 = messy, 10 = tidy)",
    y = "Social at Home (1 = quiet, 10 = social)"
  ) 
```

The Euclidean Distance is almost the same as `DistSocial`, which makes `DistTidy` almost irrelevant.

## Overview {.smaller .scrollable}

In this session you will learn:

1.  What is the underlying **idea of k-Nearest Neighbors**

2.  How similarity can be measured with **Euclidean distance**

3.  Why **scaling predictor variables** is important for some machine learning models

4.  Why the **tidymodels package** makes it easy to work with machine learning models

5.  How you can define a **recipe** to pre-process data with the `tidymodels` package

6.  How you can define a **model-design** with the `tidymodels` package

7.  How you can create a machine learning **workflow** with the `tidymodels` package

8.  How **metrics** derived from a **confusion matrix** can be used to asses prediction quality

9.  Why you have to be careful when interpreting *accuracy*, when you work with **unbalanced observations**

10. How a machine learning model can **process images** and how OCR (Optical Character Recognition) works»

## About the Wine Dataset

<br><br><br>We will work with a publicly available wine dataset[^1] containing 3,198 observations about different wines and their chemical properties.

[^1]: Cortez, Paulo, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. 2009. "Modeling Wine Preferences by Data Mining from Physicochemical Properties." Decision Support Systems 47 (4): 547--53. https://doi.org/10.1016/j.dss.2009.05.016.

Our goal is to develop a k-Nearest Neighbors model that can predict if a wine is red or white based on the wine's chemical properties.»

## Raw Observations from Wine Dataset {.scrollable}

```{r}
#| echo: true
#| output-location: fragment
library(rio)
DataWine=import("https://ai.lange-analytics.com/data/WineData.rds")
print(DataWine)
```

»

## Observations from Wine Dataset for Selected Variables {.scrollable}

### Sulfur Dioxide and Acidity

Note we use `clean_names("upper_camel")` from the `janitor` package to change all column (variable) names to UpperCamel.

```{r}
#| echo: true
#| output-location: fragment
library(tidyverse); library(rio);library(janitor)
DataWine=import("https://ai.lange-analytics.com/data/WineData.rds") |> 
  clean_names("upper_camel") |> 
  select(WineColor,Sulfur=TotalSulfurDioxide,Acidity) |> 
  mutate(WineColor=as.factor(WineColor))
print(DataWine)
```

»

## Conceptual Detour: Before Starting with k Nearest Neighbors
### Introducing a Few other Machine Learning Models with Diagrams

<br><br><br><br>

#### Let us find some eyeballing techniques that are related to various machine learning models»

## Eye Balling Techniques to Identify Red and White Wines {.scrollable}

#### try eyeballing the data

```{r}
library(tidymodels);
set.seed(876)
Split7030=initial_split(DataWine,prop=0.7,strata = WineColor)

DataTrain=training(Split7030)
DataTest=testing(Split7030) 
```

```{r WinePlot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_color_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## Eye Balling Techniques to Identify Red and White Wines

#### Horizontal Boundary

```{r WinePlotAcid8, echo=FALSE, fig.cap="Horizontal Decision Boundary for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_color_manual(values = c("red", "green","blue"))+
  geom_abline(slope = 0, intercept = 8)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'half'"
ConMatTemp$table[2]="FN: 'half'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

## Eyeballing Techniques to Identify Red and White Wines

#### Creating Subspaces Like Similar to a Decision Tree

```{r WinePlotDecTree, echo=FALSE, fig.cap="Sub-Space Boundaries for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_color_manual(values = c("red", "green","blue"))+
  geom_abline(slope = 0, intercept = 8)+
  geom_segment(x=75, xend=75, y=0, yend=8, color="black", size=1)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'most'"
ConMatTemp$table[2]="FN: 'few'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

## Eyeballing Techniques to Identify Red and White Wines

#### Using a non-linear Decision Boundary Like a Neural Network

```{r WinePlotSVM, message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Curved Decision Boundary for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_color_manual(values = c("red", "green","blue"))+
  geom_curve(aes(x=35,y=4, xend=150,yend=12), color="black", curvature=-0.2, size=1)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'most'"
ConMatTemp$table[2]="FN: 'few'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

# So, how does k Nearest Neighbors Work?

## k Nearest Neighbors k=1 {transition="fade-in zoom-out" transition-speed="slow"}
### (find the closest datapoint to the one we want to predict)
```{r WinePlotAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color»"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' acidity and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_color_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## k Nearest Neighbors k=1 {transition="zoom-in slide-out" transition-speed="slow"}
### (find the closest datapoint to the one we want to predict)
```{r WinePlotK1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Predicting  Wine Color with k-Nearest Neighbors (k=1)"}
library(latex2exp)
ggplot(DataTrain |> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
         geom_point(size=5)+
         labs(x="Total Sulfur Dioxide (mg/liter)", color="Wine Color")+
         scale_x_continuous(limits = c(66.5,70), breaks=seq(65,80,0.5))+
         scale_y_continuous(limits = c(6.7,7.5), breaks=seq(6.7,8,0.1))+
         geom_segment(aes(x = 68.5,y =6.8, xend =68.5, yend=7), color="black")+ #vert
         geom_segment(aes(x = 68.5,y =7, xend =69, yend=7), color="black")+ # horiz
         geom_segment(aes(x = 68.5,y =6.8, xend =69, yend=7), color="magenta", size=1.2)+
         scale_color_manual(values = c("red", "green", "blue"))+
  annotate(geom="text", x=68.5, y=6.9, label=TeX("$b = (Acid_p - Acid_i)$"),
              color="black", hjust=1.1)+
  annotate(geom="text", x=68.75, y=7, label=TeX("$a = (Sulfur_p - Sulfur_i)$"),
              color="black", vjust=-0.77)+
  annotate(geom="text", x=68.75, y=6.9, label="c = Dist",
              color="black", hjust=-0.17)
```

## How to calculate Euclidean Distance for Two Variables

Assume our observations have **two predictor variables** $x$ and $y$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{(x_p-x_i)^2+(y_p-y_i)^2}$$ »

## How to calculate Euclidean Distance for Three Variables

Assume our observations have **three predictor variables** $x$, $y$, and $z$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{(x_p-x_i)^2+(y_p-y_i)^2+(z_p-z_i)^2}$$ »

## How to calculate Euclidean Distance for N Variables

Assume our observations have $N$ predictor variables $v_j$ with $j=1 ... N$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{\sum_{j=1}^N(v_{p,j}-v_{i,j})^2}$$ »

## k Nearest Neighbors k=4 (for a different unknown wine){transition="fade-in zoom-out" transition-speed="slow"}
### (find the closest 4 datapoint to the one we want to predict)
```{r WinePlotAgainAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_color_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## k Nearest Neighbors k=4 (for a different unknown wine){transition="zoom-in slide-out" transition-speed="slow"}
### (find the closest 4 datapoint to the one we want to predict)
#### 4 nearest neighbors vote on "red" vs. "white"

```{r WinePlotK4, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Predicting Wine Color with k-Nearest Neighbors (k=4)"}
ggplot(DataTrain |> 
         add_row(WineColor="unknown", Acidity=7.3,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
         geom_point(size=5)+
         labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color")+
         scale_x_continuous(limits = c(66.5,70), breaks=seq(65,80,0.5))+
         scale_y_continuous(limits = c(6.7,7.5), breaks=seq(6.7,8,0.1))+
         geom_segment(aes(x = 68.5,y =7.3, xend =69, yend=7.3), color="magenta", size=1.2)+ 
         geom_segment(aes(x = 68.5,y =7.3, xend =69, yend=7), color="magenta")+ 
         geom_segment(aes(x = 68.5,y =7.3, xend =68, yend=7.2), color="magenta")+
         geom_segment(aes(x = 68.5,y =7.3, xend =68, yend=7.1), color="magenta")+
         annotate(geom="text", x=69, y=7.3, label=TeX("$N_1$"),
              color="black", vjust=-1.1)+
         annotate(geom="text", x=68, y=7.2, label=TeX("$N_2$"),
              color="black", hjust=1.57)+
         annotate(geom="text", x=68, y=7.1, label=TeX("$N_3$"),
              color="black", hjust=1.57)+
         annotate(geom="text", x=69, y=7, label=TeX("$N_4$"),
              color="black", hjust=1.57)+
  scale_color_manual(values = c("red", "green", "blue"))
```

## k Nearest Neighbors k=4 (for a different unknown wine){transition="fade-in zoom-out" transition-speed="slow"}

#### Watch the scale: g/liter vs. mg/liter. That does not look right!

```{r WinePlotAgainAgainAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color »"}
ggplot(DataTrain|> 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_color_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## A Few Common Scaling Options {.smaller}

-   **Same units**

    Divide or multiply to get the same units. This is often not possible (e.g., `Height` and `Weight`). Or it is not feasible (e.g. `Alcohol` and `StrawberryJuice` content in spiked strawberry drink))

-   **Rescaling**

    Generates a variable $y$ that is scaled to a range between 0 and 1 based on the original variable's value $x$, its minimum $x_{min}$ and its maximum $x_{max}$: $$ y= \frac{x-x_{min}}{x_{max} - x_{min}}$$

-   **Z-Score Normalization**

    Z-score normalization uses the mean ($\overline x$) and the standard deviation ($s$) of a variable to scale the variable $x$ to the variable $z$:

    $$z=\frac{x-\overline x}{s}$$»

## Time to Run k-Nearest Neighbors

The required library `tidymodels`,  `kknn` and `janitor`  are loaded in the background, and the Data we are using are already saved in the data frame `DataWine` (see below):

```{webr}
library(kableExtra)
DataWine=readRDS(url("https://ai.lange-analytics.com/data/WineData.rds")) |> 
         clean_names("upper_camel") |> 
         select(WineColor,Sulfur=TotalSulfurDioxide,Acidity) |> 
         mutate(WineColor=as.factor(WineColor))
kable(head(DataWine))
```


## Generate Training and Testing Data (Splitting):

```{webr}
set.seed(876)
Split7030=initial_split(DataWine,prop=0.7,strata = WineColor)
DataTrain=training(Split7030)
DataTest=testing(Split7030)

Split7030$in_id[1:20] #only for output; usually not needed
```

::: {.columns}

::: {.column width="35%"}
`DataWine`

```{webr}
head(Split7030$data, n=20)
```
:::

::: {.column width="32%"}
`DataTrain`

```{webr}
head(DataTrain)
```
:::

::: {.column width="32%"}
`DataTest` (not original order)

```{webr}
head(DataTest)
```
:::

:::

```{webr}
cat("Proportion of red wines in DataTrain", mean(DataTrain$WineColor=="red"))
cat("Proportion of red wines in DataTest", mean(DataTest$WineColor=="red"))
```
## Defining the Recipe
### [Click here to find a reference list for various `Step_` commands](https://recipes.tidymodels.org/reference/index.html){target="_blank"}

Recipe: Prepare Data for Analysis:

```{webr}
RecipeWine=recipe(WineColor~Acidity+Sulfur, data = DataTrain) |>
            step_naomit() |> 
            step_normalize(all_predictors())
```

Or:

```{webr}
RecipeWine=recipe(WineColor~., data = DataTrain) |>
            step_naomit() |> 
            step_normalize(all_predictors()) 
```

```{webr}
print(RecipeWine) #red output does not mean error
```

## Creating the Model-Design
### [Click here to find a reference list for various *Model- Designs* commands](https://parsnip.tidymodels.org/reference/index.html)


```{webr}
ModelDesignKNN=nearest_neighbor(neighbors = 4, weight_func = "rectangular") |>
               set_engine("kknn") |> 
               set_mode("classification")
print(ModelDesignKNN)
```

`weight_func = "rectangular"` is needed to use a *textbook version* of *k-Nearest-Neighbors*. It can be omitted for research.

## Adding Recipe & Model-Design to Workflow and fit() it with the Training Data

Putting it all together in a **fitted** `workflow`:

```{webr}
WFModelWine=workflow() |> 
            add_recipe(RecipeWine) |>
            add_model(ModelDesignKNN) |> 
            fit(DataTrain)
print(WFModelWine)
```
## The Prediction Procedure

How to use the **fitted** `workflow` which contains the training data (`DAtaTrain`)to predict the wine color for the wines in the testing dataset:<br><br>

1. Start with observation $i=1$ from `DataTest` (the first observation).<br><br>
2. Take observation $i$ from `DataTest` and use `Acidity` and `Sulfur` to calculate the *Euclidean distance* to **each** of the observations of `DataTrain`.<br><br>
3. Isolate the 4 observations with the smallest *Euclidean distance* and use the majority of their wine color as a prediction for observation $i$ from `DataTest` (in case of a par, decide randomly).<br><br>
4. Increase $i$ by one (i.e., take the next observation from `DataTest`) and go to step 2 (until all `DataTest` observations are processed).


## Predicting Wine Color of Testing Data Using the `predict()` command

Predicting with the fitted workflow using `predict()` (not exactly helpful!):

```{webr}
predict(WFModelWine, DataTest)
```

## Predicting Wine Color of Testing Data Using the `augment()` command

Predicting with the fitted workflow using `augment()`.  The  `augment()` command *predicts* and then *augments* `DataTest` with the predictions:

```{webr}
DataPredWithTestData=augment(WFModelWine, DataTest)
head(DataPredWithTestData)
```

## Having a Data Frame with `truth` and `estimate`, We can Calculate Performance Metrics

Confusion Matrix:

```{webr}
ConfMatrixWine=conf_mat(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
print(ConfMatrixWine)
```

## Reading the Confusion Matrix {.smaller}

```{r}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 436"
ConMatTemp$table[2]="FN: 44"
ConMatTemp$table[3]="FP: 46"
ConMatTemp$table[4]="TN: 434"
print(ConMatTemp)
```

-   The **positive class** (wine is *predicted* `red`) is in the **first row** and shows all wines  that are *predicted* `positive`  (`red`). 436  are  *predicted* correctly (**TP: True Positives**), and 46 are *predicted* incorrectly as `red` but are `white`  (**FP: False Positives**).<br><br>

-   The **negative class** (wine is *predicted* `white`) is in the **second row** and shows all wines that are *predicted* `negative`  (`white`). 44 are *predicted* incorrectly as `white` but are `red` (**FP: False Negatives**), and 434  are  *predicted* correctly as `white`(**TP: True Negatives**).<br><br>

- Remember: 
  - in `tidymodels` *first class* is always *positive*

  - when determining `TP`, `FP`, `FN`, `TN`, approach matrix from the *prediction side*

**Accuracy:** Number of wines on diagonal/number of all wines ($\frac{TP+TN}{TP+TN+FP+FN}$):


```{webr}
accuracy(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
```

## tidymodels Modeling Pipeline {.smaller}


1. Split the Data into *Training* and *Testing Data*

```{r}
#| eval: false
#| echo: true
set.seed(876)
Split7030=initial_split(DataToAnalyzeName, prop=0.7, strata=OutcomeVarName)
DataTrain=training(Split7030)
DataTest=testing(Split7030) 
```

2. Create Recipe

```{r}
#| eval: false
#| echo: true
RecipeName=recipe(OutcomeVarName~Pred1VarName+Pred2VarName, data=DataTrain) |>
           step_naomit() |> 
           step_Name()
```

3 Create Model-Design

```{r}
#| eval: false
#| echo: true
ModelDesignName=MachLearnModelName() |>
                set_engine("PackageName") |> 
                set_mode("classification or regression")
```

4. Create Workflow and Fit to Training Data

```{r}
#| eval: false
#| echo: true
WFModelName=workflow() |> 
            add_recipe(RecipeName) |>
            add_model(ModelDesignName) |> 
            fit(DataTrain)
```

5. Predict with Workflow and Augment Testing Data with Predictions

```{r}
#| eval: false
#| echo: true
DataTestWithPred=augment(WFModelName, DataTest)
```


6. Assess Predictive Quality with Metrics

```{r}
#| eval: false
#| echo: true
MetricsCommand(DataTestWithPred, truth=OutcomeVarName,
                        estimate=.pred_class or estimate=.pred)
```

## **Warning: Be careful with the Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

Dr. Nebulous offers a **97% Machine Learning Gambling Prediction**. Here is how it works: Gamblers can buy a prediction for a fee of \$5. Dr. Nebulous will then run his famous machine learning model and send a closed envelope with the prediction. The gambler is supposed to open the envelope in the casino, right before placing a bet of \$100 on a number in roulette. The envelope contains a message that states either "You will win" or "You will lose", which allows the gambler to act accordingly by either bet or not bet.

Dr. Nebulous claims that a "clinical trial" of 1000 volunteers, who opened the envelope after they had bet on a number in roulette, shows an accuracy of 97.3%.

**How could Dr. Nebulous have such a precise model?**

## **Warning: Be careful with the Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

The trick is Dr. Nebulous's machine learning model uses the *naive prognosis*: It always predicts "You will lose".

Here is the confusion matrix from the 1,000 volunteers trial:

```{r echo=FALSE}
ConMatNebulous=ConMatTemp

rownames(ConMatNebulous$table)=c("Win","Lose")
colnames(ConMatNebulous$table)=c("Win","Lose")
ConMatNebulous$table[1]=0
ConMatNebulous$table[2]=27
ConMatNebulous$table[3]=0
ConMatNebulous$table[4]=973
print(ConMatNebulous)
```

Roulette has 37 numbers to bet on. Chance to win is: $\frac{1}{37}=0.027$.

Out of the 1000 volunteers, 27 are expected to win $\frac{1}{37} /cdot 1000=27$, and the others (1000-27=973) are expected to lose.

$$Accuracy=\frac{0+973}{1000}=0.973$$

## **Warning: Be careful with the Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

```{r echo=FALSE}
rownames(ConMatNebulous$table)=c("Win","Lose")
colnames(ConMatNebulous$table)=c("Win","Lose")
ConMatNebulous$table[1]=0
ConMatNebulous$table[2]=27
ConMatNebulous$table[3]=0
ConMatNebulous$table[4]=973
print(ConMatNebulous)
```

However, when we look at the correct positive and the correct negative rate separately, we see that Dr. Nebulous' accuracy rate (although correct) makes little sense.

-   The correct negative rate (**specificity**) is 100%

-   The correct positive rate (**sensitivity**) is zero (out of the 27 winners, all were falsely predicted as "You will lose").

**This example shows: When interpreting the confusion matrix, you must look at accuracy, sensitivity, and specificity simultaneously**

## Using  accuracy(), sensitivity() and specificity() to Asses Prediction Quality

### `accuracy()`

```{webr}
conf_mat(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
accuracy(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
``` 

$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

- Remember: 
  - `TP` and `TN` are the elements on the main diagonal of the confusion matrix


## Using  accuracy(), sensitivity() and specificity() to Asses Prediction Quality

### `sensitivity()`

```{webr}
conf_mat(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
sensitivity(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
``` 

$Sensitivity=\frac{TP}{TP+FN}$ (first column)

- Remember: 
  - in `tidymodels` *first class* is always *positive*, which is relevant for *Sensitivity*
  - when determining *Sensitivity* and *Specificity* approach matrix from the *Truth* side

## Using  accuracy(), sensitivity() and specificity() to Asses Prediction Quality

### `specificity()`

```{webr}
conf_mat(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
specificity(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
``` 

$Specificity=\frac{TN}{TN+FP}$ (second column)

- Remember: 
  - in `tidymodels` *second class* is always *negative*, which is relevant for *Specificity*
  - when determining *Sensitivity* and *Specificity* approach matrix from the *Truth* side

## Using All Variables in The Wine Dataset to Predict Wine Color

**Can we improve by using all predictors?**

[Try the Interactive Exercise: Chapter 4 – k-Nearest Neighbors: Predicting Wine Color](https://aibookslides.netlify.app/liveinteractsectaibook/exercises){target="_blank"}

::: footer
See: [Exercises from AI Book](https://aibookslides.netlify.app/liveinteractsectaibook/exercises)
:::

## Project: Design a Machine Learning Workflow for Optical Character Recognition

You will develop a machine learning model based on *k-Nearest Neighbors* to recognize handwritten digits from images.

You will use the MNIST dataset, a standard dataset for image recognition in machine learning (60,000 images for training and 10,000 images for testing). Developed by LeCun, Cortes, and Burges (2010) based on two datasets from handwritten digits obtained from Census workers and high school students.

[Work with the Interactive Exercise: Chapter 4 – k-Nearest Neighbors Project: Read Images with Handwritten Digits](https://aibookslides.netlify.app/liveinteractsectaibook/exercises){target="_blank"}