---
title: "k Nearest Neighbors"
author:
  name: "Carsten Lange"
  email: "clange@cpp.edu"
  affiliation: "Cal Poly, Pomona"

format: 
  live-revealjs:
    theme: [moon,../CustomCL.scss]
    smaller: true
    controls: true
    chalkboard:
      theme: whiteboard
      boardmarker-width: 2.7
    incremental: false
    scrollable: true
    footer: "<a href='https://ai.lange-analytics.com/htmlbook/KNearNeigh.html' target='_blank'>Textbook </a> "
   
  
webr:
  packages:
    - ggplot2
  
 

engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}

---

## Before we Begin Let Us Do A Thought Experiment

**I need three volunteers. Everybody else, please follow along on one computer in groups of three.**

Imagine the three of you are moving into a shared apartment.

There are two bedrooms: two people will share a big room, and one person will have a small room alone.

**To minimize conflicts, we want the two most similar people to share the two rooms.**

Please rate yourselves on a **scale from 1 to 10** for the two criteria below:

1. How tidy you are (**1=messy, 10= tidy**)

2. How social you are at home (**1=quiet, 10=very social**)

**We will treat your answers as data points and use Euclidean distance to decide who should share.**

##  Thought Experiment: Data Entry

For `Student1`, `Student2`, and `Student3`, let us enter the names of the three volunteer student. Then we replace the`___` with their scores (in the correct order of `Student1`, `Student2`, and `Student3`).

Afterwards, click <span style="background-color: #75AADB; padding: 3px; color: white;">Run Code</span>
```{webr}
#| warning: false
#| message: false
library(kableExtra)
DataRoomMates <- data.frame(
  name = c("Student 1", "Student 2", "Student 3"),
  Tidy = c(1, 8, 3),   # 1 = messy, 10 = very tidy
  Social = c(7, 2,5 )     # 1 = quiet, 10 = very social
)
kable(DataRoomMates)
```

## Thought Experiment: Plotting the Data

```{webr}
library(ggplot2)
ggplot(DataRoomMates, aes(x = Tidy, y = Social, label = name)) +
  geom_point(size = 4) +
  geom_text(vjust = -1) +
  scale_x_continuous(limits = c(0.5, 10.5), breaks=seq(1:10)) +
  scale_y_continuous(limits = c(0.5, 10.5), breaks=seq(1:10)) +
  coord_fixed(ratio = 1) +   # <-- makes the plot square
  labs(
    title = "Roommate Matching Using Euclidean Distance",
    x = "Tidy (1 = messy, 10 = tidy)",
    y = "Social at Home (1 = quiet, 10 = social)"
  ) 
```

$$EucDist^2=DiffTidy^2+DiffSocial^2$$

$$EucDist=sqrt(DiffTidy^2+DiffSocial^2)$$

## Compute Euclidean distances


```{webr}
MatrixDistance <- dist(DataRoomMates[, c("Tidy", "Social")], method = "euclidean")
print(as.matrix(MatrixDistance))
```

**Testing the distance between the first and the second student:**
$$EucDist^2=DiffTidy^2+DiffSocial^2$$

$$EucDist=sqrt(DiffTidy^2+DiffSocial^2)$$

```{webr}
DiffTidy=DataRoomMates[[1,2]]-DataRoomMates[[2,2]]
cat("Diff. Tidy between the first and the second student:",DiffTidy)
DiffSocial=DataRoomMates[[1,3]]-DataRoomMates[[2,3]]
cat("Diff. Social between the first and the second student:",DiffSocial)
EucDist=sqrt((DiffTidy^2+DiffSocial^2))
cat("Euclidean Distance=",EucDist)
```

## Scale for All Variables

#### It is very important that all variables have the same scale

Like here, both variables are in a range from 1 to 10.

#### What happens if we scale `Social` from 10 to 100 instead from 1 to 10

```{webr}
DataRoomMates$Social=DataRoomMates$Social*10
kable(DataRoomMates)
```

## The Influence of Social Compared to Tidy becomes very Small

```{webr}
library(ggplot2)
ggplot(DataRoomMates, aes(x = Tidy, y = Social, label = name)) +
  geom_point(size = 4) +
  geom_text(vjust = -1) +
  scale_x_continuous(limits = c(1, 10), breaks=seq(1,10,9)) +
  scale_y_continuous(limits = c(10, 100), breaks=seq(10,100,10)) +
  coord_fixed(ratio = 1) +   # <-- makes the plot square
  labs(
    title = "Roommate Matching Using Euclidean Distance",
    x = "Tidy (1 = messy, 10 = tidy)",
    y = "Social at Home (1 = quiet, 10 = social)"
  ) 
```

The Euclidean Distance is almost the same as `DistSocial`, which makes `DistTidy` almost irrelevant.

## Overview {.smaller .scrollable}

In this session you will learn:

1.  What is the underlying **idea of k-Nearest Neighbors**

2.  How similarity can be measured with **Euclidean distance**

3.  Why **scaling predictor variables** is important for some machine learning models

4.  Why the **tidymodels package** makes it easy to work with machine learning models

5.  How you can define a **recipe** to pre-process data with the `tidymodels` package

6.  How you can define a **model-design** with the `tidymodels` package

7.  How you can create a machine learning **workflow** with the `tidymodels` package

8.  How **metrics** derived from a **confusion matrix** can be used to asses prediction quality

9.  Why you have to be careful when interpreting *accuracy*, when you work with **unbalanced observations**

10. How a machine learning model can **process images** and how OCR (Optical Character Recognition) worksÂ»

## About the Wine Dataset

<br><br><br>We will work with a publicly available wine dataset[^1] containing 3,198 observations about different wines and their chemical properties.

[^1]: Cortez, Paulo, AntÃ³nio Cerdeira, Fernando Almeida, Telmo Matos, and JosÃ© Reis. 2009. "Modeling Wine Preferences by Data Mining from Physicochemical Properties." Decision Support Systems 47 (4): 547--53. https://doi.org/10.1016/j.dss.2009.05.016.

Our goal is to develop a k-Nearest Neighbors model that can predict if a wine is red or white based on the wine's chemical properties.Â»

## Raw Observations from Wine Dataset {.scrollable}

```{r}
#| echo: true
#| output-location: fragment
library(rio)
DataWine=import("https://ai.lange-analytics.com/data/WineData.rds")
print(DataWine)
```

Â»

## Observations from Wine Dataset for Selected Variables {.scrollable}

### Sulfor Dioxide and Acidity

Note we use `clean_names("upper_camel")` from the `janitor` package to change all column (variable) names to UpperCamel.

```{r}
#| echo: true
#| output-location: fragment
library(tidyverse); library(rio);library(janitor)
DataWine=import("https://ai.lange-analytics.com/data/WineData.rds") %>% 
  clean_names("upper_camel") %>% 
  select(WineColor,Sulfur=TotalSulfurDioxide,Acidity) %>% 
  mutate(WineColor=as.factor(WineColor))
print(DataWine)
```

Â»

## Before Starting with k Nearest Neighbors

<br><br><br><br>

#### Let us find some eyeballing techniques that are related to various machine learning modelsÂ»

## Eye Balling Techniques to Identify Red and White Wines {.scrollable}

#### try eyeballing the data

```{r}
library(tidymodels);
set.seed(876)
Split7030=initial_split(DataWine,prop=0.7,strata = WineColor)

DataTrain=training(Split7030)
DataTest=testing(Split7030) 
```

```{r WinePlot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_color_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## Eye Balling Techniques to Identify Red and White Wines

#### Horizontal Boundary

```{r WinePlotAcid8, echo=FALSE, fig.cap="Horizontal Decision Boundary for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' accidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_colour_manual(values = c("red", "green","blue"))+
  geom_abline(slope = 0, intercept = 8)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'half'"
ConMatTemp$table[2]="FN: 'half'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

## Eyeballing Techniques to Identify Red and White Wines

#### Creating Subspaces Like Similar to a Decision Tree

```{r WinePlotDecTree, echo=FALSE, fig.cap="Sub-Space Boundaries for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' accidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_colour_manual(values = c("red", "green","blue"))+
  geom_abline(slope = 0, intercept = 8)+
  geom_segment(x=75, xend=75, y=0, yend=8, color="black", size=1)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'most'"
ConMatTemp$table[2]="FN: 'few'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

## Eyeballing Techniques to Identify Red and White Wines

#### Using a non-linear Decision Boundary Like a Neural Network

```{r WinePlotSVM, message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Curved Decision Boundary for Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)", color="Wine Color", alt="A point plot of the wines' acidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,25))+
  scale_y_continuous(breaks=seq(4,16,2))+
  scale_colour_manual(values = c("red", "green","blue"))+
  geom_curve(aes(x=35,y=4, xend=150,yend=12), color="black", curvature=-0.2, size=1)+
  theme(legend.position = c(0.9, 0.8))
```

#### Confusion Matrix

```{r echo=FALSE}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 'most'"
ConMatTemp$table[2]="FN: 'few'"
ConMatTemp$table[3]="FP: 'few'"
ConMatTemp$table[4]="TN: 'most'"
print(ConMatTemp)
```

# So, how does k Nearest Neighbors Work?

## k Nearest Neighbors k=1 {transition="fade-in zoom-out" transition-speed="slow"}

```{r WinePlotAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine ColorÂ»"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' accidity and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_colour_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## k Nearest Neighbors k=1 {transition="zoom-in slide-out" transition-speed="slow"}

```{r WinePlotK1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Predicting  Wine Color with k-Nearest Neighbors (k=1)"}
library(latex2exp)
ggplot(DataTrain %>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
         geom_point(size=5)+
         labs(x="Total Sulfur Dioxide (mg/liter)", color="Wine Color")+
         scale_x_continuous(limits = c(66.5,70), breaks=seq(65,80,0.5))+
         scale_y_continuous(limits = c(6.7,7.5), breaks=seq(6.7,8,0.1))+
         geom_segment(aes(x = 68.5,y =6.8, xend =68.5, yend=7), color="black")+ #vert
         geom_segment(aes(x = 68.5,y =7, xend =69, yend=7), color="black")+ # horiz
         geom_segment(aes(x = 68.5,y =6.8, xend =69, yend=7), color="magenta", size=1.2)+
         scale_colour_manual(values = c("red", "green", "blue"))+
  annotate(geom="text", x=68.5, y=6.9, label=TeX("$b = (Acid_p - Acid_i)$"),
              color="black", hjust=1.1)+
  annotate(geom="text", x=68.75, y=7, label=TeX("$a = (Sulfur_p - Sulfur_i)$"),
              color="black", vjust=-0.77)+
  annotate(geom="text", x=68.75, y=6.9, label="c = Dist",
              color="black", hjust=-0.17)
```

## How to calculate Euclidean Distance for Two Variables

Assume our observations have **two predictor variables** $x$ and $y$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{(x_p-x_i)^2+(y_p-y_i)^2}$$ Â»

## How to calculate Euclidean Distance for Three Variables

Assume our observations have **three predictor variables** $x$, $y$, and $z$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{(x_p-x_i)^2+(y_p-y_i)^2+(z_p-z_i)^2}$$ Â»

## How to calculate Euclidean Distance for N Variables

Assume our observations have $N$ predictor variables $v_j$ with $j=1 ... N$. We compare the unknown point $p$ to one of the points from the training data (e,g., point $i$): $$Dist_i=\sqrt{\sum_{j=1}^N(v_{p,j}-v_{i,j})^2}$$ Â»

## k Nearest Neighbors k=4 (for a different unknown wine){transition="fade-in zoom-out" transition-speed="slow"}

```{r WinePlotAgainAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' accidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_colour_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## k Nearest Neighbors k=4 (for a different unknown wine){transition="zoom-in slide-out" transition-speed="slow"}

#### 4 nearest neighbors vote on "red" vs. "white"

```{r WinePlotK4, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Predicting Wine Color with k-Nearest Neighbors (k=4)"}
ggplot(DataTrain %>% 
         add_row(WineColor="unknown", Acidity=7.3,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
         geom_point(size=5)+
         labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color")+
         scale_x_continuous(limits = c(66.5,70), breaks=seq(65,80,0.5))+
         scale_y_continuous(limits = c(6.7,7.5), breaks=seq(6.7,8,0.1))+
         geom_segment(aes(x = 68.5,y =7.3, xend =69, yend=7.3), color="magenta", size=1.2)+ 
         geom_segment(aes(x = 68.5,y =7.3, xend =69, yend=7), color="magenta")+ 
         geom_segment(aes(x = 68.5,y =7.3, xend =68, yend=7.2), color="magenta")+
         geom_segment(aes(x = 68.5,y =7.3, xend =68, yend=7.1), color="magenta")+
         annotate(geom="text", x=69, y=7.3, label=TeX("$N_1$"),
              color="black", vjust=-1.1)+
         annotate(geom="text", x=68, y=7.2, label=TeX("$N_2$"),
              color="black", hjust=1.57)+
         annotate(geom="text", x=68, y=7.1, label=TeX("$N_3$"),
              color="black", hjust=1.57)+
         annotate(geom="text", x=69, y=7, label=TeX("$N_4$"),
              color="black", hjust=1.57)+
  scale_colour_manual(values = c("red", "green", "blue"))
```

## k Nearest Neighbors k=4 (for a different unknown wine){transition="fade-in zoom-out" transition-speed="slow"}

#### Watch the scale: g/liter vs. mg/liter. That does not look right!

```{r WinePlotAgainAgainAgain, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Acidity and Total Sulfur Dioxide Related to Wine Color Â»"}
ggplot(DataTrain%>% 
         add_row(WineColor="unknown", Acidity=6.8,Sulfur=68.5),aes(y=Acidity,x=Sulfur,color=WineColor))+
  labs(x="Total Sulfur Dioxide (mg/liter)", y="Acidity (tartaric acid in g/liter)",color="Wine Color", alt="A point plot of the wines' accidity 
                                                    and total sulfur dioxide")+
  geom_point(size=1, alpha=0.35)+
  geom_point(aes(x=68.5, y=6.8), size=3, color="green")+
  scale_x_continuous(breaks=seq(0,300,50))+
  scale_colour_manual(values = c("red", "green","blue"))+
  theme(legend.position = c(0.9, 0.8))
```

## A Few Common Scaling Options {.smaller}

-   **Same units**

    Divide or multiply to get the same units. This is often not possible (e.g., `Height` and `Weight`). Or it is not feasible (e.g. `Alcohol` and `StrawberryJuice` content in spiked strawberry drink))

-   **Rescaling**

    Generates a variable $y$ that is scaled to a range between 0 and 1 based on the original variable's value $x$, its minimum $x_{min}$ and its maximum $x_{max}$: $$ y= \frac{x-x_{min}}{x_{max} - x_{min}}$$

-   **Z-Score Normalization**

    Z-score normalization uses the mean ($\overline x$) and the standard deviation ($s$) of a variable to scale the variable $x$ to the variable $z$:

    $$z=\frac{x-\overline x}{s}$$Â»

## Time to Run k-Nearest Neighbors

#### Loading Data and Selecting Variables

```{r}
#| echo: true
#| output-location: fragment
library(tidyverse); library(rio);library(janitor)
DataWine=import("https://ai.lange-analytics.com/data/WineData.rds") %>% 
         clean_names("upper_camel") %>% 
         select(WineColor,Sulfur=TotalSulfurDioxide,Acidity) %>% 
  mutate(WineColor=as.factor(WineColor))
print(DataWine)
```

Â»

## Time to Run k-Nearest Neighbors {.scrollable}

Generate Training and Testing Data (Splitting):

```{r}
#| echo: true
#| output-location: fragment
library(tidymodels);
set.seed(876)
Split7030=initial_split(DataWine,prop=0.7,strata = WineColor)

DataTrain=training(Split7030)
DataTest=testing(Split7030)
print(DataTrain)
print(DataTest)
```

## Time to Run k-Nearest Neighbors {.scrollable}
### [Click here to find a reference list for various `Step_` commands](https://recipes.tidymodels.org/reference/index.html){target="_blank"}

Recipe: Prepare Data for Analysis:

```{r}
#| echo: true
RecipeWine=recipe(WineColor~Acidity+Sulfur, data = DataTrain) %>%
            step_naomit() %>% 
            step_normalize(all_predictors())
```

Or:

```{r }
#| echo: true
RecipeWine=recipe(WineColor~., data = DataTrain) %>%
            step_naomit() %>% 
            step_normalize(all_predictors()) 

```

```{r}
#| echo: true
#| output-location: fragment
print(RecipeWine)
```

## Time to Run k-Nearest Neighbors {.scrollable}
### [Click here to find a reference list for various `Step_` commands](https://parsnip.tidymodels.org/reference/index.html)

Creating a Model Design:

```{r}
#| echo: true
#| output-location: fragment
ModelDesignKNN=nearest_neighbor(neighbors = 4, weight_func = "rectangular") %>%
               set_engine("kknn") %>% 
               set_mode("classification")
print(ModelDesignKNN)
```

## Time to Run k-Nearest Neighbors

Putting it all together in a **fitted** `workflow`:

```{r}
#| echo: true
#| output-location: fragment
#| 
WFModelWine=workflow() %>% 
             add_recipe(RecipeWine) %>%
             add_model(ModelDesignKNN) %>% 
             fit(DataTrain)
print(WFModelWine)
```
## Time to Run k-Nearest Neighbors

How to use the **fitted** `workflow` to predict the wine color for the wines in the testing dataset:

1. Start with observation $i=1$ from `DataTest` (the first observation).
2. Take observation $i$ from `DataTest` and use `Acidity` and `Sulfur` to calculate the Euclidean distance to **each** of the observations of `DataTrain`.
2. Isolate the 4 observations with the smallest Euclidean distance and use the majority of their wine color as a prediction for observation $i$ from `DataTest` (in case of a par, decide randomly).
4. Increase $i$ by one (i.e., take the next observation from `DataTest`) and go to step 2 (until all `DataTest` observations are processed).


## Time to Run k-Nearest Neighbors

Predicting with the fitted workflow using `predict()` (not exactly helpful!):

```{r}
#| echo: true
#| output-location: fragment
predict(WFModelWine, DataTest)
```

## Time to Run k-Nearest Neighbors

Predicting with the fitted workflow using `augment()` which *augments* `DataTest` with the predictions:

```{r}
#| echo: true
#| output-location: fragment
DataPredWithTestData=augment(WFModelWine, DataTest)
head(DataPredWithTestData)
```

## Having a Data Frame with `truth` and `esimate` we can calculate **performance metrics**

Confusion Matrix:

```{r}
#| echo: true
#| output-location: fragment
ConfMatrixWine=conf_mat(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
print(ConfMatrixWine)
```

## Reading the Confusion Matrix

```{r}
DataTemp=tibble(A=as.factor(c(1,1,3)),B=as.factor(c(1,1,3)))
ConMatTemp=conf_mat(DataTemp, truth = A, estimate = B)


rownames(ConMatTemp$table)=c("Red Wine","White Wine")
colnames(ConMatTemp$table)=c("Red Wine","White Wine")
ConMatTemp$table[1]="TP: 436"
ConMatTemp$table[2]="FN: 44"
ConMatTemp$table[3]="FP: 46"
ConMatTemp$table[4]="TN: 434"
print(ConMatTemp)
```

-   The **positive class** (wine is *predicted* `red`) is in the **first row** and shows all wines that are *predicted* `positive`  (`red`). 436  are  *predicted* correctly (**TP: True Positives**), and 46 are *predicted* incorrectly as `red` but are `white`  (**FP: False Positives**).

-   The **negative class** (wine is *predicted* `white`) is in the **second row** and shows all wines that are *predicted* `negative`  (`white`). 44 are *predicted* incorrectly as `white` but are `red` (**FP: False Negatives**), and 434  are  *predicted* correctly as `white`(**TP: True Negatives**).

. . .

**Accuracy:** Number of wines on diagonal/number of all wines:

. . .

```{r}
#| echo: true
#| output-location: fragment
accuracy(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
```


## **Warning: Be careful with the Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

Dr. Nebulous offers a **97% Machine Learning Gambling Prediction**. Here is how it works: Gamblers can buy a prediction for a fee of \$5. Dr. Nebulous will then run his famous machine learning model and send a closed envelope with the prediction. The gambler is supposed to open the envelope in the casino, right before placing a bet of \$100 on a number in roulette. The envelope contains a message that states either "You will win" or "You will lose", which allows the gambler to act accordingly by either bet or not bet.

Dr. Nebulous claims that a "clinical trial" of 1000 volunteers, who opened the envelope after they had bet on a number in roulette, shows an accuracy of 97.3%.

**How could Dr. Nebulous have such a precise model?**

## **Warning: Be careful with thethe Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

The trick is Dr. Nebulous's machine learning model uses the *naive prognosis*: It always predicts "You will lose".

Here is the confusion matrix from the 1,000 volunteers trial:

```{r echo=FALSE}
ConMatNebulous=ConfMatrixWine

rownames(ConMatNebulous$table)=c("Win","Lose")
colnames(ConMatNebulous$table)=c("Win","Lose")
ConMatNebulous$table[1]=0
ConMatNebulous$table[2]=27
ConMatNebulous$table[3]=0
ConMatNebulous$table[4]=997
print(ConMatNebulous)
```

Roulette has 37 numbers to bet on. Chance to win is: $\frac{1}{37}=0.027$.

Out of the 1000 volunteers, 27 are expected to win, and 973 are expected to lose.

$$Accuracy=\frac{0+973}{1000}=0.973$$

## **Warning: Be careful with the Accuracy Rate**

#### The Story of Dr. Nebulous's Gamblers System

```{r echo=FALSE}
ConMatNebulous=ConfMatrixWine

rownames(ConMatNebulous$table)=c("Win","Lose")
colnames(ConMatNebulous$table)=c("Win","Lose")
ConMatNebulous$table[1]=0
ConMatNebulous$table[2]=27
ConMatNebulous$table[3]=0
ConMatNebulous$table[4]=997
print(ConMatNebulous)
```

However, when we look at the correct positive and the correct negative rate separately, we see that Dr. Nebulous' accuracy rate (although correct) makes little sense.

-   The correct negative rate (**specificity**) is 100%

-   The correct positive rate (**sensitivity**) is zero (out of the 27 winners, all were falsely predicted as "You will lose").

. . .

**This example shows: When interpreting the confusion matrix, you must look at accuracy, sensitivity, and specificity simultaneously**

## Time to Run k-Nearest Neighbors ðŸ¤“

`accuracy()`, `sensitivity()` and `specificity()` for the wine data:

```{r}
#| echo: true
#| output-location: fragment
accuracy(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
```

. . .

```{r}
#| echo: true
#| output-location: fragment
sensitivity(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
```

. . .

```{r}
#| echo: true
#| output-location: fragment
specificity(DataPredWithTestData, truth = WineColor, estimate = .pred_class)
```

. . .

Can we improve by using all predictors.Â»

::: footer
See: [Exercise from AI Book](https://ai.lange-analytics.com/exc/?file=04-KNearNeighExerc200.Rmd)
:::

# Project: Design a Machine Learning Workflow for Optical Character Recognition Â»

## MNIST Data Set

You will develop a machine learning model based on k-Nearest Neighbors to recognize handwritten digits from images.

You will use the MNIST dataset, a standard dataset for image recognition in machine learning (60,000 images for training and 10,000 images for testing). Developed by LeCun, Cortes, and Burges (2010) based on two datasets from handwritten digits obtained from Census workers and high school students.

We will use only the first 500 images of the original MNIST dataset to speed up the *k-Nearest Neighbors* model's training time.

## Visualization of the First Six Images from the MNIST Data Set

```{r}
#|echo: false
FctPlotImages=function(Data){
  MnistLong = Data %>% 
    mutate(instance = row_number()) %>%
    gather(pixel, value, -Label, -instance) %>%
    tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
    mutate(pixel = pixel - 2,
           x = pixel %% 28,
           y = 28 - pixel %/% 28) 
  Plot=MnistLong %>% ggplot(aes(x, y, fill = value)) +
    geom_tile() +
    facet_wrap(~ instance + Label)+
    scale_fill_gradient(
      low = "#000000",
      high = "#FFFFFF",
      guide = "colourbar",
      aesthetics = "fill")+ coord_fixed()
  return(Plot)}


Mnist4PlotAndTable = import("Data/Mnist4PlotAndTable.rds") 

FctPlotImages(Mnist4PlotAndTable)
```

## How a Image is Stored in the Mnist Dataset

![Image of a Handwritten Nine](Images/CodedNine.PNG){width="1500"}

The image has 28 rows and 28 columns. Each of the 784 cells (pixels) holds a value between 0 (black) and 255 (white)

## How a Image is Stored in the Mnist Dataset

::: columns
::: {.column width="50%"}
![Image of a Handwritten Nine](Images/CodedNine.PNG)


:::

::: {.column width="50%"}
- Pixel values for a single image are not stored in a table. Ohterwise we would end-up with a table containing tables.
- Pixel values are stored as one row for each image. 
- Concatenating the 28 rows of an image into one row with 28*28=784 cells (pixels)
:::
::::

## Three Rows from the Data Frame of the MNIST Dataset {.smaller .scollable}

```{r}
#| echo: true
#| output-location: fragment
print(Mnist4PlotAndTable[1:3,1:784])
```

## Go to Project in Book ðŸ¤“

### Build your own OCR system.Â»


::: footer
See: [RandRStudioScriptPart1.R script300](https://ai.lange-analytics.com/exc/?file=04-KNearNeighExerc300.Rmd)
:::
